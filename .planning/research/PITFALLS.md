# Pitfalls Research

**Domain:** Simulated AI Livestream Experience -- multiple personas, chat bots, goal-driven narrative, scene transitions
**Researched:** 2026-02-17
**Confidence:** HIGH (experience/UX patterns), MEDIUM (Gemini structured output specifics, Unity scene loading edge cases)

---

## Critical Pitfalls

### Pitfall 1: The "Finish First" Trap -- User Feels Ignored While Aya Completes Her Response

**What goes wrong:**
The core design intentionally has Aya finish her current response/animation before addressing the user's push-to-talk input. This is meant to feel like a real streamer who naturally wraps up a thought before pivoting to a viewer. But without deliberate acknowledgment mechanics, the user experiences a dead zone: they speak, nothing visible happens, and seconds pass. Research from Disney's "Let Me Finish First" study found that users who had to wait for an agent to finish its speech found the experience "awkward and frustrating." The 300ms rule in voice AI (AssemblyAI research) establishes that silences beyond 4 seconds degrade quality of experience substantially.

**Why it happens:**
The QueuedResponseController's 5-state machine (Connecting, Idle, Recording, Reviewing, Playing) already buffers audio during the "Reviewing" state, but the livestream scene introduces a new wrinkle: Aya is already talking (to chat bots, about her art) when the user speaks. The user's input queues behind Aya's current turn. If Aya is mid-monologue about her characters, the user might wait 15-20 seconds before hearing acknowledgment. This is qualitatively different from the QueuedResponse sample where Aya was idle before the user spoke.

**How to avoid:**
1. **Visual acknowledgment within 500ms.** The moment the user's push-to-talk is released, show a visible indicator in the chat/UI: "Aya noticed your message" or a subtle glance animation. This is cheap and breaks the perceived silence.
2. **Aya acknowledges verbally at the end of her current sentence, not her current monologue.** Instead of finishing a 3-paragraph response, Aya should reach a sentence boundary, then say something like "Oh hold on -- [user], let me get back to you" before finishing her thought briefly. This requires sentence-level granularity in the "finish first" logic, not turn-level.
3. **Cap the wait time.** If Aya has been speaking for more than 8-10 seconds after the user's input arrives, interrupt at the next sentence boundary regardless. The user's patience degrades exponentially after 4 seconds of perceived silence.
4. **Typing indicator in chat.** Show "Aya is thinking about your question..." in the chat feed immediately. This is the same pattern every messaging app uses and it works because it confirms receipt.
5. **Filler animation.** Research on mitigating response delays (2025) found that natural conversational fillers (chin-touch, "Hmm, let me think...") improve perceived response time significantly, while generic loading indicators had minimal effect. A subtle "glance at camera" or "pen tap" animation when user input arrives signals attention without interrupting speech.

**Warning signs:**
- User says something and then waits >5 seconds with no feedback
- Playtesters report "she didn't hear me" or "is the mic broken?"
- Users stop using push-to-talk after the first failed attempt
- User spams the push-to-talk button trying to get attention

**Phase to address:**
The QueuedResponse/push-to-talk integration phase. The "finish first" priority system needs the sentence-boundary awareness and visual acknowledgment wired in from the start, not added as a polish pass.

---

### Pitfall 2: Chat Bots That Feel Like a Script, Not a Community

**What goes wrong:**
The hybrid chat bot system (scripted messages + Gemini structured output for dynamic responses) creates two failure modes that both destroy the illusion:

- **Too scripted:** Messages arrive at metronomic intervals (every 15 seconds on the dot), with no variation in length, no emoji personality, no typos, no reactions to what Aya just said. The chat feels like a slideshow, not a conversation.
- **Too dynamic:** Every bot message is generated by Gemini, which adds latency (1-3 seconds per structured output call) and produces eerily fluent prose. Real chat messages are short, slangy, full of abbreviations and reactions. AI-generated chat reads like a blog post.

**Why it happens:**
Developers model chat bots as "message generators" rather than "simulated people." A real Twitch chat has rhythm: bursts of messages after something exciting happens, lulls during quiet moments, messages that reference each other, running jokes, and users typing at different speeds. Scripted messages are timed to a clock, not to events. Gemini-generated messages are too well-written.

**How to avoid:**
1. **Event-driven timing, not clock-driven.** Chat bots should react to what Aya says, not to a timer. When Aya mentions a character name, 2-3 bots should react within 1-4 seconds (staggered). During quiet drawing moments, chat should slow to one message every 20-30 seconds. This requires subscribing to Aya's output transcription events and matching trigger keywords.
2. **Per-bot personality quirks in the ScriptableObject.** Each bot config needs: message length tendency (short/medium/long), emoji usage frequency, typing speed (affects delay before message appears), vocabulary level, favorite phrases/catchphrases. These constraints shape the output whether scripted or generated.
3. **Message imperfection.** Scripted messages should include deliberate lowercase, abbreviations ("omg," "lol," "wait fr?"), and occasional typos. Generated messages should have a post-processing pass that shortens them and adds personality noise.
4. **Burst-and-lull pattern.** Model chat activity as waves. After Aya says something interesting (detected via transcription keywords or goal events), schedule 3-5 bot messages in a 2-8 second burst with random stagger. Then silence for 10-20 seconds. Never uniform spacing.
5. **Cross-referencing between bots.** Occasionally have Bot B respond to Bot A's message, not to Aya. "@ChatUser23 exactly!! her art style is so good" creates the illusion of a community, not isolated message generators.

**Warning signs:**
- All chat messages are roughly the same length (a dead giveaway)
- Chat activity does not change when Aya says something exciting vs mundane
- No bot ever references another bot's message
- Messages read like paragraphs, not chat messages
- Perfectly regular timing between messages

**Phase to address:**
Chat bot system design phase. The ScriptableObject config for bot personas and the timing/scheduling system must be designed together. Retrofitting event-driven timing onto a clock-driven system is painful.

---

### Pitfall 3: Goal Steering That Feels Like a Pushy Salesperson

**What goes wrong:**
Aya has a narrative arc: talk about her art, introduce her characters, build toward a movie clip reveal. The ConversationalGoals system uses priority-based urgency framing ("[HIGH PRIORITY - Act on this urgently]"). If the goal priority escalates too aggressively, Aya starts shoehorning the movie clip into every response: "That's a great question about my favorite color! Speaking of favorites, I've been working on this movie clip..." This breaks immersion instantly.

Convai's research explicitly identifies this as "narrative stagnation" risk: without careful calibration, either the conversation meanders aimlessly (too passive) or the AI forces transitions unnaturally (too aggressive). The balance is the hardest design problem in the entire milestone.

**Why it happens:**
The GoalManager's `ComposeGoalInstruction()` injects urgency framing directly into the system instruction. When a goal is HIGH priority, the instruction says "actively steer the conversation toward this goal. Bring it up naturally but persistently." But "naturally but persistently" is contradictory at HIGH priority -- the AI interprets "persistently" as "in every response." Additionally, the Gemini Live API does not support mid-session system instruction updates (as noted in PersonaSession.SendGoalUpdate()), so priority escalation requires reconnection or relies on the model's interpretation of accumulated context.

**How to avoid:**
1. **Time-based priority escalation with wide intervals.** Start all narrative goals at LOW priority. After 3-5 minutes of natural conversation, escalate to MEDIUM. Only escalate to HIGH after 8-10 minutes or when chat bots have naturally set up the topic. Never start at HIGH.
2. **Use chat bots as narrative catalysts, not Aya's system instruction.** Instead of telling Aya "steer toward the movie clip," have a chat bot ask "omg aya when are you gonna show us the thing you've been working on??" This makes the steering feel like audience demand, not AI agenda. Aya can respond naturally to a direct question.
3. **Rewrite urgency framing.** The HIGH priority instruction should not say "bring it up in every response." Instead: "If the conversation naturally touches on [topic], this is the moment to transition. If not, wait for a better opening." The model needs permission to NOT steer.
4. **Multiple mini-goals, not one big goal.** Instead of one goal "reveal the movie clip," create a chain: (1) mention you've been drawing a lot lately, (2) talk about the character you're drawing, (3) mention you've been working on something special, (4) when asked, reveal the movie clip. Each mini-goal is LOW priority and completes naturally, creating a breadcrumb trail.
5. **Avoid the word "urgently" in any goal framing.** Even MEDIUM priority should say "look for natural openings" not "work toward this when natural" (which the AI reads as "force it to be natural").

**Warning signs:**
- Aya mentions the movie clip within the first 2 minutes
- Aya pivots to the goal topic mid-sentence when answering an unrelated question
- Playtesters say "she keeps talking about the same thing"
- The transition to the movie clip feels abrupt rather than earned
- Aya's responses sound like a marketing pitch

**Phase to address:**
Conversational goals / narrative flow phase. The goal chain design and urgency framing language must be iterated through playtesting, not just code review. This is a prompt engineering problem as much as a systems problem.

---

### Pitfall 4: Gemini Structured Output for Chat Bots -- Latency and Cost Spiral

**What goes wrong:**
Using Gemini's `generateContent` with `response_mime_type: "application/json"` to generate dynamic chat bot responses seems elegant: define a schema with `sender`, `message`, `emoji`, and get structured bot responses. But each call has overhead:
- **Latency:** Even though Google states structured output adds "minimal latency" to the API call, the base latency for `generateContent` is still 500ms-2s for Flash models. If you call it for every bot response, chat feels sluggish.
- **Cost:** At $0.10/1M input tokens for Flash-Lite, generating 60 bot messages per 10-minute stream costs pennies. But the prompt context (Aya's current state, conversation history, bot personalities) grows with each call. Sending 2000+ tokens of context per bot message is wasteful.
- **Schema complexity:** Requesting multiple bot responses in a single call (batch) is efficient but introduces ordering problems -- the model might generate all messages with the same tone, or create artificial dialogue that no human chat would produce.

**Why it happens:**
Developers default to "call the API for everything" without considering that 80% of bot messages can be pre-authored. The scripted/dynamic split is not just for cost -- it is for quality. Pre-authored messages with personality quirks are often better than generated ones because they can be hand-tuned.

**How to avoid:**
1. **80/20 scripted-to-dynamic ratio.** Pre-author 30-50 messages per bot persona covering common reactions ("omg that's so cool," "wait what did she just say," "can you show us [character name]?"). Randomly select from these for ambient chat. Only use Gemini for responses to user input or for reactions to unexpected Aya statements.
2. **Batch bot responses in a single call.** When you do call Gemini, request 3-5 bot messages at once with a schema like `{messages: [{sender, text, delay_seconds}]}`. This amortizes the latency across multiple messages and lets the model create a realistic conversation fragment.
3. **Use Gemini Flash-Lite (not Flash or Pro).** For short chat messages, Flash-Lite at $0.10/1M input is 5x cheaper than Flash. The quality difference is negligible for "lol nice" and "omg that's amazing."
4. **Cache the context window.** Gemini supports context caching. If the bot personas and Aya's system instruction are stable across calls, cache them to reduce input token costs and latency.
5. **Fire-and-forget with delay.** Generate bot responses asynchronously and inject them into the chat with a 1-3 second delay to simulate "typing." This hides the API latency behind perceived typing time.

**Warning signs:**
- Bot messages appear with noticeable delay after the triggering event
- API costs exceed $1/hour of stream simulation
- All bot messages in a batch sound similar or reference each other unnaturally
- Bot responses are longer than typical chat messages (>30 words)
- Rate limiting errors from Gemini API during high-activity moments

**Phase to address:**
Chat bot system implementation phase. The scripted message pool and Gemini call strategy must be designed together. Starting with "everything is dynamic" and then optimizing later wastes API costs during development too.

---

### Pitfall 5: Unity Scene Loading Destroys the Livestream State

**What goes wrong:**
The movie clip is triggered as a Unity scene load when Aya's narrative goal completes. If loaded with `LoadSceneMode.Single`, it destroys the current scene -- killing the WebSocket connection (PersonaSession.OnDestroy fires, calling Disconnect), the chat UI, all bot state, and the audio pipeline. After the clip plays, returning to the livestream requires a full reconnection to Gemini, which means 2-5 seconds of dead time, loss of conversation history, and a jarring experience. Even with `LoadSceneMode.Additive`, there are problems: duplicate AudioListeners, EventSystems, and camera conflicts.

**Why it happens:**
Unity's scene management was designed for game levels, not for overlay content within a persistent session. The PersonaSession's OnDestroy calls `_client.Dispose()`, closing the WebSocket. The movie clip scene likely has its own Camera and AudioListener. Loading it additively means two of each, which Unity warns about ("There are 2 audio listeners in the scene").

**How to avoid:**
1. **Never use LoadSceneMode.Single for the movie clip.** The livestream scene must persist. Use additive loading exclusively.
2. **Disable the livestream camera and AudioListener during clip playback.** Before loading the clip scene, disable the main camera and AudioListener. Re-enable after unloading the clip scene. This avoids the duplicate warnings and ensures the clip's camera/audio takes over cleanly.
3. **Do NOT destroy PersonaSession during clip playback.** Keep the WebSocket alive but muted. Suppress mic input and audio output during the clip. When the clip ends, Aya can resume naturally: "So, what did you think?" without a reconnection delay.
4. **Pre-load the clip scene.** Use `SceneManager.LoadSceneAsync` with `allowSceneActivation = false` to pre-load the scene in the background while Aya is still talking. When the goal triggers, activate the pre-loaded scene instantly -- no loading screen, no pause.
5. **State preservation checklist:** Before clip activation, save: current goal state, chat history (for UI), bot activity timers, audio playback position. After clip, restore all of these. The user should feel like the stream paused, not restarted.
6. **Handle the transition gracefully in-narrative.** Aya should say something like "OK chat, I want to show you something -- check this out!" and then the clip plays. When it ends, Aya says "So yeah, that's what I've been working on!" This frames the scene load as part of the experience, not a technical interruption.

**Warning signs:**
- Black screen or loading spinner between livestream and clip
- WebSocket reconnection visible in the console after clip plays
- Chat history is gone after returning from the clip
- Audio pops or cuts at the transition boundary
- "There are 2 audio listeners in the scene" warning in console

**Phase to address:**
Scene loading / movie clip trigger phase. The additive loading strategy, pre-loading, and state preservation must be designed before implementation. Retrofitting persistence onto a Single-mode load is a rewrite.

---

### Pitfall 6: Multi-Persona Coherence Collapse -- Aya Contradicts Herself or Chat Bots

**What goes wrong:**
Aya is driven by Gemini Live (WebSocket, real-time audio). Chat bots are driven by scripted messages + Gemini structured output (REST API, text-only). These are independent systems with no shared state. Aya might say "I'm drawing Luna right now" while a scripted bot message says "I love how you drew Kai earlier" -- creating a continuity error. Worse, if the user asks Aya about something a bot said, Aya has no context about bot messages because they were generated outside her session.

**Why it happens:**
Aya's Gemini session has its own context window that only sees: system instruction, user audio/text, and Aya's own responses. Bot messages are never injected into Aya's context. The bots and Aya are in separate conversational universes that happen to share a UI.

**How to avoid:**
1. **Inject bot messages into Aya's context.** When a bot says something significant (especially a question directed at Aya), send it as a text message to Aya's Gemini session via `_client.SendText("[ChatBot42 in chat]: omg aya show us the movie clip!")`. This puts the bot message into Aya's conversation history so she can reference it.
2. **Give Aya a "chat awareness" system instruction section.** Include: "You are a livestreamer. You can see a chat window with messages from viewers. Sometimes viewers will ask you questions or make comments. You may reference what viewers say."
3. **Limit what bots can assert.** Scripted bot messages should react to Aya ("that's so cool!") not assert new facts ("I heard you're working on a movie"). Only Gemini-generated bot messages (which can be conditioned on Aya's transcription) should reference specific things Aya said.
4. **Shared state object.** Maintain a lightweight state object (current topic Aya is discussing, character name mentioned, current activity) that both Aya's system instruction and bot message generation can reference. Update it from Aya's output transcription events.

**Warning signs:**
- Bot references something Aya hasn't mentioned yet
- Aya doesn't know what a bot just said when the user asks about it
- Bot mentions a character by wrong name or wrong context
- Aya repeats something a bot already said, word for word

**Phase to address:**
This spans multiple phases: bot system design (scripted message constraints), Aya's system instruction (chat awareness), and the integration phase (context injection via SendText). Must be architected early even if wired up later.

---

### Pitfall 7: The Uncanny Valley of Chat Timing

**What goes wrong:**
Real livestream chat has a very specific temporal texture. Messages appear in bursts after exciting moments, with variable delays that correspond to typing speed. Each person types at a different speed. There are pauses. Sometimes two people say the same thing simultaneously. Sometimes there is dead silence for 30 seconds. Simulated chat that does not replicate this texture feels immediately wrong, even if the content is perfect.

Research on livestream chat UX (GetStream, 2025) found that for high-message-traffic chats, using "auto" scroll (instant jump) is better than "smooth" scroll, and that slow mode (throttling) is critical for digestibility. But for a simulated stream with 4-6 bots, the problem is the opposite: not enough messages, and they need to feel individually typed.

**Why it happens:**
Developers schedule bot messages with `Invoke("PostMessage", delay)` or coroutines with fixed waits. Every message takes exactly the same time to "type" regardless of length. There is no variation in inter-message timing. The result is a metronome, not a chat room.

**How to avoid:**
1. **Per-message typing delay based on message length.** A 5-word message takes 1-2 seconds to "type." A 20-word message takes 3-5 seconds. Add random jitter (+/- 30%) to prevent regularity.
2. **Stagger bot responses to the same event.** When Aya says something exciting, Bot A reacts in 0.5-1.5s (the fast typer), Bot B in 2-4s (the moderate typer), Bot C in 4-8s (the slow typer). These delays should be defined per-bot in the ScriptableObject config.
3. **"Dead chat" moments are natural.** Do not fill every silence with bot messages. Real streams have lulls. After a burst of activity, schedule 15-30 seconds of no messages. This makes the bursts feel impactful by contrast.
4. **Simultaneous messages.** Occasionally (10% of bursts), have two bots post within 200ms of each other. This mimics the real "chat goes crazy" pattern when something exciting happens.
5. **Scroll behavior.** Auto-scroll (snap to bottom) when new messages arrive, unless the user has scrolled up to read history. Show a "new messages" indicator if scrolled up. This is the standard Twitch/YouTube chat pattern.

**Warning signs:**
- All messages are exactly evenly spaced
- Short messages and long messages appear after the same delay
- Chat activity is constant regardless of what Aya is doing
- No moments of "dead chat" between bursts
- Chat scrolls smoothly instead of snapping (feels sluggish at low message volume)

**Phase to address:**
Chat bot timing system phase, which should be designed alongside the chat UI. The timing model and the UI scroll behavior are tightly coupled -- getting one right without the other creates a disconnect.

---

## Technical Debt Patterns

Shortcuts that seem reasonable but create long-term problems.

| Shortcut | Immediate Benefit | Long-term Cost | When Acceptable |
|----------|-------------------|----------------|-----------------|
| All bot messages from Gemini (no scripted pool) | Simpler code, no message authoring | 500ms+ latency per message, API costs, messages too polished | Never for ambient chat. Only for reactive responses to user input |
| Single Gemini call per bot message | Simple 1:1 mapping | Latency multiplied by bot count (5 bots = 5 API calls = 2.5-10s total) | Never. Batch multiple bot responses per call |
| LoadSceneMode.Single for movie clip | Simple scene management | Destroys WebSocket, chat state, requires full reconnection | Never. Always additive |
| Injecting ALL bot messages into Aya's context | Perfect coherence | Floods Aya's context window, increases token costs, confuses the model | Never for all messages. Only inject significant ones (questions to Aya, topic changes) |
| Fixed goal priority (start at HIGH) | Goal completes faster | Aya sounds pushy, breaks immersion | Never. Always escalate from LOW over time |
| No typing indicator for Aya's response to user | Less UI code | User thinks mic is broken during "finish first" wait | Never. Always show acknowledgment |
| Clock-based bot timing (every N seconds) | Simple timer implementation | Chat feels robotic, no relationship to content | Only acceptable as a Phase 1 placeholder, must be replaced with event-driven timing |
| Hardcoded bot personas in code | Faster to prototype | Cannot iterate on personality without recompilation | Early prototyping only. Move to ScriptableObjects before playtesting |

## Integration Gotchas

Common mistakes when connecting components in the livestream system.

| Integration | Common Mistake | Correct Approach |
|-------------|----------------|------------------|
| Aya PersonaSession + Chat Bot System | Chat bots and Aya run in parallel with no shared context | Inject significant bot messages into Aya's session via SendText. Maintain shared state object for current topic/activity |
| User Push-to-Talk + Aya's Current Turn | Interrupting Aya immediately (standard barge-in behavior) | Queue user input. Show visual acknowledgment. Let Aya reach sentence boundary, then pivot |
| Movie Clip Scene + Livestream Scene | LoadSceneMode.Single destroys everything | Additive loading. Pre-load with allowSceneActivation=false. Disable livestream camera/AudioListener during clip |
| Goal Escalation + Time-Based Triggers | Timer fires regardless of conversation state (Aya mid-sentence about something else) | Check conversation state before escalating. Only escalate during natural pauses or topic transitions |
| Bot Gemini Calls + Aya Gemini Session | Same API key, potential rate limiting conflicts | Use separate API keys or stagger bot calls. Aya's WebSocket session is persistent; bot calls are ephemeral REST. They should not contend |
| Chat UI Scroll + New Messages | ScrollView does not auto-scroll when new messages are added via code | Force scroll-to-bottom in the next frame (UI Toolkit requires a frame delay for layout update). Use `schedule.Execute(() => scrollView.scrollOffset = new Vector2(0, scrollView.contentContainer.layout.height))` |
| Audio Playback + Movie Clip Audio | Both playing simultaneously during transition | Mute/pause AudioPlayback before activating clip scene. Resume after clip unloads |

## Performance Traps

Patterns that work in testing but fail in production or extended play.

| Trap | Symptoms | Prevention | When It Breaks |
|------|----------|------------|----------------|
| Gemini API call per bot message | Chat lag, API throttling | Batch bot responses. 80% scripted, 20% dynamic | >10 bot messages per minute during active moments |
| Unbounded chat history in UI | Memory growth, ScrollView sluggishness | Cap visible messages at 100-200. Remove oldest when limit reached | After 10+ minutes of active chat |
| No message pooling in chat UI | GC spikes when creating/destroying chat message UI elements | Pool VisualElements. Reuse by swapping text/name/color | After 50+ messages in a session |
| Output transcription accumulation in StringBuilder | Memory growth from _ttsTextBuffer and _functionCallBuffer | Already cleared on TurnComplete (existing code handles this). Verify it also clears on interruption during long monologues | Only if interruption handler has bugs |
| Pre-loading movie clip scene too early | Memory pressure from two full scenes in memory | Pre-load only when goal reaches MEDIUM priority (a few minutes before trigger), not at session start | Only if clip scene has heavy assets (large textures, meshes) |

## Security Mistakes

Domain-specific security issues for a simulated livestream.

| Mistake | Risk | Prevention |
|---------|------|------------|
| Bot personas or scripted messages contain real user data | Privacy violation if bot "personas" are based on real people | Use clearly fictional bot names and personas. Document that all chat users are simulated |
| API key exposed in chat UI or debug logs | Key compromise, billing abuse | Never log API keys. Sanitize debug output. Use AIEmbodimentSettings (already in Resources, not in scene) |
| User audio sent to Gemini during movie clip | Unnecessary API usage, potential privacy concern | Disable mic capture during clip playback. Call StopListening() before clip activation |
| No content safety on bot messages | Generated bot messages could contain inappropriate content | Apply safety settings to Gemini structured output calls for bot messages. Filter scripted messages during authoring |

## UX Pitfalls

Common user experience mistakes in livestream simulation.

| Pitfall | User Impact | Better Approach |
|---------|-------------|-----------------|
| Chat messages appear instantly (no typing simulation) | Feels robotic, breaks immersion | Add per-message delay based on message length and bot's "typing speed" personality trait |
| Aya talks continuously with no pauses | Overwhelming, user cannot find a moment to push-to-talk | Build natural pauses into Aya's system instruction: "Pause occasionally to let your chat react." 3-5 seconds of silence every 30-60 seconds |
| Movie clip plays with no buildup | Jarring transition, no emotional payoff | Aya builds anticipation: "I want to show you something I've been working on..." with 5-10 second tease before trigger |
| All bots have the same "voice" (similar vocabulary, length, emoji use) | Chat feels like one person with multiple accounts | Differentiate bots with distinct personality configs: emoji-heavy bot, one-word-reaction bot, question-asker bot, lurker-who-rarely-speaks bot |
| User cannot tell the difference between bots and real functionality | Confusion about what is interactive and what is decorative | Push-to-talk should be visually distinct from bot chat. Maybe a different input area, microphone icon, or distinct color. Bots are in the chat feed; user input is a separate action |
| Chat scroll is smooth-animated at low message volume | Looks floaty and artificial when one message arrives every 10 seconds | Use instant scroll (snap) when message volume is low (<1 msg/sec). Only use smooth scroll during rapid bursts |
| No visual indicator of Aya's "state" (talking, listening, drawing, thinking) | User does not know when to talk or what Aya is doing | Show Aya's state in the UI: a subtle indicator like "LIVE -- Aya is drawing" or "Aya is listening..." or "Aya is responding..." |

## "Looks Done But Isn't" Checklist

Things that appear complete but are missing critical pieces.

- [ ] **Chat bot system:** Often missing cross-bot references -- verify at least 10% of messages reference another bot's message or Aya's specific statement
- [ ] **Goal-driven narrative:** Often missing the "permission to NOT steer" -- verify Aya can have 3+ exchanges without mentioning the movie clip when the user is asking about unrelated topics
- [ ] **Finish-first priority:** Often missing the visual acknowledgment -- verify user sees feedback within 500ms of releasing push-to-talk, even if Aya is still talking
- [ ] **Movie clip transition:** Often missing audio crossfade -- verify no audio pop/silence at the boundary between Aya's speech and clip audio, and between clip end and Aya's resumption
- [ ] **Chat timing:** Often missing dead chat moments -- verify there are at least 2-3 periods of 15+ seconds with no bot messages in a 10-minute session
- [ ] **Bot personalities:** Often missing the "lurker" -- verify at least one bot speaks rarely (every 2-3 minutes) to create realistic participation variance
- [ ] **Scene return:** Often missing state restoration -- verify chat history, bot timers, and goal state survive the movie clip round-trip
- [ ] **Aya's pauses:** Often missing natural silence -- verify Aya pauses for 3+ seconds at least twice per minute to let chat react and user speak
- [ ] **User input during clip:** Often missing input suppression -- verify push-to-talk is disabled and mic is muted during movie clip playback
- [ ] **Narrative payoff:** Often missing emotional buildup -- verify Aya has at least 2-3 tease moments before the movie clip reveal ("I've been working on something special...")

## Recovery Strategies

When pitfalls occur despite prevention, how to recover.

| Pitfall | Recovery Cost | Recovery Steps |
|---------|---------------|----------------|
| User feels ignored (finish-first too slow) | LOW | Add visual acknowledgment, cap wait time at sentence boundary. No architecture change needed -- just UI feedback and a timer check |
| Chat bots feel scripted | MEDIUM | Retrofit event-driven timing onto existing timer system. Requires hooking into OnOutputTranscription for trigger keywords. May need to redesign bot scheduling |
| Goal steering too aggressive | LOW | Adjust goal framing text in GoalManager. Reduce escalation speed. Purely prompt engineering -- no code change |
| Goal steering too passive (never reaches movie clip) | LOW | Add time-based fallback: if 15 minutes pass, escalate to HIGH regardless. Add bot catalyst messages. Prompt engineering + one timer |
| Gemini structured output latency | MEDIUM | Shift to higher scripted ratio. Batch bot messages. Cache context. Requires restructuring the bot message pipeline but not the UI |
| Scene load destroys session | HIGH | Requires switching from Single to Additive loading, adding state preservation, managing duplicate cameras/AudioListeners. Architectural change |
| Coherence collapse (Aya vs bots) | MEDIUM | Add SendText injection for bot messages, add shared state object. Requires wiring but does not break existing architecture |
| Chat timing too uniform | LOW | Adjust timing parameters in bot configs. Add jitter. No architecture change |

## Pitfall-to-Phase Mapping

How roadmap phases should address these pitfalls.

| Pitfall | Prevention Phase | Verification |
|---------|------------------|--------------|
| Finish-first feels unresponsive (#1) | Push-to-talk / QueuedResponse integration | Playtester reports <5s perceived wait with visible acknowledgment. No "is the mic broken?" feedback |
| Chat bots feel scripted (#2) | Chat bot system design | Side-by-side comparison: can an observer distinguish simulated chat from a recording of real Twitch chat for 30 seconds? |
| Goal steering too pushy (#3) | Narrative flow / conversational goals | 10-minute playtest: Aya does NOT mention the movie clip in first 3 minutes. Transition feels earned |
| Gemini structured output overhead (#4) | Chat bot implementation | Bot messages appear within 1-3 seconds of triggering event. API costs < $0.50 per 10-minute session |
| Scene load destroys state (#5) | Movie clip / scene loading | Chat history, goal state, and WebSocket connection survive clip round-trip. No reconnection in console |
| Multi-persona coherence (#6) | Integration / system wiring | User asks Aya about something a bot said, Aya can reference it. No factual contradictions in 10-minute session |
| Chat timing uncanny valley (#7) | Chat bot timing system | Variable inter-message delays. Burst-and-lull pattern visible. Dead chat moments present |

## Sources

- [Google Research -- DialogLab: Multi-party AI group conversation dynamics](https://research.google/blog/beyond-one-on-one-authoring-simulating-and-testing-dynamic-human-ai-group-conversations/) -- MEDIUM confidence. Informed turn-taking and coherence pitfalls. Published 2025.
- [Frontiers in AI -- Multi-party turn-taking in Murder Mystery games](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1582287/full) -- MEDIUM confidence. Speaker selection mechanisms, dialogue breakdown patterns.
- [AssemblyAI -- The 300ms rule for voice AI latency](https://www.assemblyai.com/blog/low-latency-voice-ai) -- HIGH confidence. Established latency thresholds for perceived responsiveness.
- [arXiv -- Mitigating Response Delays in Free-Form Conversations with LLM-powered IVAs](https://arxiv.org/html/2507.22352v1) -- HIGH confidence. Filler strategies, 4-second degradation threshold, "natural conversational fillers improve perceived response time."
- [Convai -- Narrative Design for AI NPCs](https://convai.com/blog/convai-narrative-design) -- MEDIUM confidence. Goal-driven steering challenges, "narrative stagnation" risk.
- [GetStream -- 7 UX Best Practices for Livestream Chat](https://getstream.io/blog/7-ux-best-practices-for-livestream-chat/) -- HIGH confidence. Scroll behavior, message pacing, slow mode, virtualized lists.
- [Gemini API -- Structured Output documentation](https://ai.google.dev/gemini-api/docs/structured-output) -- HIGH confidence. Schema design, streaming support, "minimal latency" claim for structured output overhead.
- [Gemini API Pricing 2026](https://www.aifreeapi.com/en/posts/gemini-api-pricing-2026) -- MEDIUM confidence. Flash-Lite at $0.10/1M input tokens, cost optimization strategies.
- [Unity Discussions -- Additive scene loading and duplicate AudioListeners](https://discussions.unity.com/t/avoiding-multiple-event-systems-audio-listeners-etc-with-additive-scene-loading/866174) -- HIGH confidence. Standard Unity issue with well-known workarounds.
- Existing codebase analysis: PersonaSession.cs, QueuedResponseController.cs, GoalManager.cs, GeminiLiveClient.cs -- HIGH confidence. Direct source code reading.
- Disney Research "Let Me Finish First" study (2024) -- MEDIUM confidence. Referenced via secondary sources, not directly verified. Key finding: users found wait-to-finish "awkward and frustrating."
- [NN/g -- Response Time Limits](https://www.nngroup.com/articles/response-times-3-important-limits/) -- HIGH confidence. Classic HCI research on perceived responsiveness.

---
*Pitfalls research for: AI Embodiment v1.0 Livestream Experience*
*Researched: 2026-02-17*
