---
phase: 06-sample-scene-and-integration
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/AyaChatUI.cs
  - Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/AyaSampleController.cs
autonomous: true

must_haves:
  truths:
    - "Chat log displays Aya messages, user transcriptions, and system event messages with distinct styling"
    - "Push-to-talk works via spacebar (Input System) and on-screen UI button (PointerDown/Up)"
    - "Three function calls (emote, start_movie, start_drawing) are registered before Connect with correct schemas"
    - "Pre-recorded intro plays on a separate AudioSource before live session starts"
    - "Conversational goal is injected after 3 exchange turns, not at session start"
    - "Speaking indicator visually toggles when AI starts/stops speaking"
  artifacts:
    - path: "Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/AyaChatUI.cs"
      provides: "UI Toolkit chat log controller with message appending, speaking glow, status updates, and PTT button wiring"
      exports: ["AyaChatUI"]
      min_lines: 80
    - path: "Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/AyaSampleController.cs"
      provides: "Main controller: function registration, intro coroutine, push-to-talk polling, goal injection after warm-up"
      exports: ["AyaSampleController"]
      min_lines: 120
  key_links:
    - from: "AyaChatUI.cs"
      to: "AyaPanel.uxml"
      via: "Q<> element queries by name"
      pattern: "Q<.*>\\(\"(chat-log|persona-name|speaking-indicator|status-label|ptt-button)\"\\)"
    - from: "AyaChatUI.cs"
      to: "PersonaSession events"
      via: "event subscriptions in OnEnable"
      pattern: "_session\\.On(OutputTranscription|InputTranscription|AISpeaking|StateChanged|TurnComplete)"
    - from: "AyaSampleController.cs"
      to: "PersonaSession.RegisterFunction"
      via: "function registration before Connect"
      pattern: "_session\\.RegisterFunction"
    - from: "AyaSampleController.cs"
      to: "PersonaSession.AddGoal"
      via: "goal injection after exchange count threshold"
      pattern: "_session\\.AddGoal"
    - from: "AyaSampleController.cs"
      to: "PersonaSession.StartListening/StopListening"
      via: "Keyboard.current.spaceKey polling in Update"
      pattern: "Keyboard\\.current.*spaceKey"
---

<objective>
Create the two C# scripts that form the Aya Live Stream sample: AyaChatUI (UI Toolkit controller) and AyaSampleController (main controller with function calls, intro, push-to-talk, and goal injection).

Purpose: These scripts are the core sample code that demonstrates the full AI Embodiment pipeline. They are pure consumer code -- they use PersonaSession's public API without extending the library.
Output: Two MonoBehaviour scripts ready to be attached to GameObjects in the sample scene.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-sample-scene-and-integration/06-RESEARCH.md
@.planning/phases/06-sample-scene-and-integration/06-01-SUMMARY.md

@Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs
@Packages/com.google.ai-embodiment/Runtime/PersonaConfig.cs
@Packages/com.google.ai-embodiment/Runtime/FunctionCallContext.cs
@Packages/com.google.ai-embodiment/Runtime/FunctionRegistry.cs
@Packages/com.google.ai-embodiment/Runtime/GoalPriority.cs
@Packages/com.google.ai-embodiment/Runtime/SessionState.cs
@Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/UI/AyaPanel.uxml
</context>

<tasks>

<task type="auto">
  <name>Task 1: AyaChatUI -- UI Toolkit chat log controller</name>
  <files>Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/AyaChatUI.cs</files>
  <action>
Create `AyaChatUI.cs` in the `AIEmbodiment.Samples` namespace. This MonoBehaviour manages the UIDocument and provides methods for the controller to log messages and update UI state.

**SerializeField references:**
- `UIDocument _uiDocument` -- the UI Toolkit document (assigned in scene Inspector)
- `PersonaSession _session` -- the persona session (for event subscriptions and Config access)

**Private cached element references** (queried in OnEnable from `_uiDocument.rootVisualElement`):
- `ScrollView _chatLog` -- queried by `Q<ScrollView>("chat-log")`
- `Label _nameLabel` -- queried by `Q<Label>("persona-name")`
- `Label _statusLabel` -- queried by `Q<Label>("status-label")`
- `Button _pttButton` -- queried by `Q<Button>("ptt-button")`
- `VisualElement _speakingIndicator` -- queried by `Q("speaking-indicator")`

**Private state tracking:**
- `Label _currentAyaMessage` -- tracks the current streaming Aya message label for incremental text updates (output transcription streams in chunks that REPLACE the full text, not append)
- `Label _currentUserMessage` -- tracks the current streaming user message label (same replacement pattern)
- `bool _isUserSpeaking` -- tracks whether we're in a user speaking turn (to know when to create vs update user message)

**OnEnable (subscribe to events, query elements, set up PTT button):**
1. Query all elements from `_uiDocument.rootVisualElement` using the names above
2. Set `_nameLabel.text` to `_session.Config.displayName` (shows "Aya" from the PersonaConfig)
3. Subscribe to PersonaSession events:
   - `_session.OnOutputTranscription += HandleAITranscription` -- streaming AI text (replaces full text each chunk)
   - `_session.OnInputTranscription += HandleUserTranscription` -- streaming user STT (replaces full text each chunk)
   - `_session.OnAISpeakingStarted += () => SetSpeakingGlow(true)`
   - `_session.OnAISpeakingStopped += HandleAISpeakingStopped`
   - `_session.OnUserSpeakingStarted += HandleUserSpeakingStarted`
   - `_session.OnUserSpeakingStopped += HandleUserSpeakingStopped`
   - `_session.OnStateChanged += HandleStateChanged`
   - `_session.OnTurnComplete += HandleTurnComplete`
4. Register PTT button callbacks:
   - `_pttButton.RegisterCallback<PointerDownEvent>(e => _session.StartListening())`
   - `_pttButton.RegisterCallback<PointerUpEvent>(e => _session.StopListening())`

**OnDisable (unsubscribe):**
Unsubscribe all event handlers registered in OnEnable to prevent leaks.

**HandleAITranscription(string text):**
- Output transcription delivers the FULL accumulated text each time (not incremental deltas)
- If `_currentAyaMessage` is null, create a new Label with class "msg-aya", add to `_chatLog`, store as `_currentAyaMessage`
- Set `_currentAyaMessage.text = $"Aya: {text}"`
- Call auto-scroll (see below)

**HandleUserTranscription(string text):**
- Same replacement semantics as AI transcription
- If `_currentUserMessage` is null, create a new Label with class "msg-user", add to `_chatLog`, store as `_currentUserMessage`
- Set `_currentUserMessage.text = $"You: {text}"`
- Call auto-scroll

**HandleTurnComplete():**
- Set `_currentAyaMessage = null` (next AI response starts a fresh message)
- Auto-scroll

**HandleAISpeakingStopped():**
- `SetSpeakingGlow(false)`

**HandleUserSpeakingStarted():**
- `_isUserSpeaking = true`
- `_currentUserMessage = null` (new user turn starts a fresh message)

**HandleUserSpeakingStopped():**
- `_isUserSpeaking = false`
- `_currentUserMessage = null` (finalize the user message)

**HandleStateChanged(SessionState state):**
- Update status label text based on state:
  - `Connecting` -> "Connecting..."
  - `Connected` -> "Live! Hold SPACE to talk."
  - `Disconnecting` -> "Disconnecting..."
  - `Disconnected` -> "Disconnected."
  - `Error` -> "Error occurred."

**SetSpeakingGlow(bool speaking):**
- If speaking: `_speakingIndicator.AddToClassList("indicator--speaking")`
- If not: `_speakingIndicator.RemoveFromClassList("indicator--speaking")`

**Public method: LogSystemMessage(string message):**
- Create a new Label with text `message`, add class "msg-system"
- Add to `_chatLog`
- Auto-scroll

**Public method: SetStatus(string text):**
- Set `_statusLabel.text = text`

**Auto-scroll helper (private):**
```csharp
private void AutoScroll()
{
    _chatLog.schedule.Execute(() =>
    {
        _chatLog.scrollOffset = new Vector2(0, _chatLog.contentContainer.layout.height);
    });
}
```
This defers the scroll by one frame so layout is recalculated first (Pitfall 5 from research).

**Using directives needed:**
- `using UnityEngine`
- `using UnityEngine.UIElements`
- `using AIEmbodiment` (for PersonaSession, SessionState)
  </action>
  <verify>
Verify file exists and compiles conceptually: all Q<> queries match UXML element names from Plan 01 (chat-log, persona-name, speaking-indicator, status-label, ptt-button). Verify all PersonaSession events referenced exist in the API. Verify namespace is AIEmbodiment.Samples.
  </verify>
  <done>
AyaChatUI.cs subscribes to PersonaSession events, manages streaming text display for both AI and user transcriptions (replacement semantics), toggles speaking indicator glow, handles PTT button via PointerDown/Up, logs system messages, and auto-scrolls the chat log.
  </done>
</task>

<task type="auto">
  <name>Task 2: AyaSampleController -- main controller with functions, intro, PTT, goals</name>
  <files>Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/AyaSampleController.cs</files>
  <action>
Create `AyaSampleController.cs` in the `AIEmbodiment.Samples` namespace. This is the main MonoBehaviour that orchestrates the sample scene: registers function calls, plays the intro, manages push-to-talk keyboard input, and injects the conversational goal after warm-up.

**SerializeField references:**
- `PersonaSession _session` -- the core session component
- `AyaChatUI _chatUI` -- the UI controller (for LogSystemMessage and SetStatus)
- `AudioSource _introAudioSource` -- SEPARATE AudioSource for pre-recorded intro (NOT the same as AudioPlayback's AudioSource -- see Pitfall 3)
- `AudioClip _introClip` -- the pre-recorded intro audio clip (can be null for testing without audio)

**Private state:**
- `bool _liveMode = false` -- tracks whether intro is done and live session is active
- `int _exchangeCount = 0` -- counts AI turn completions for goal activation timing
- `bool _goalActivated = false` -- prevents double goal injection

**Constants:**
- `const int WarmUpExchanges = 3` -- number of exchanges before goal activates (Claude's discretion per CONTEXT.md)

**Start():**
1. Register all three functions BEFORE Connect (critical -- FunctionRegistry freezes on BuildTools inside Connect, see Pitfall 4):
   - Call `RegisterFunctions()` (private method, see below)
2. Subscribe to `_session.OnTurnComplete += HandleTurnComplete` for exchange counting
3. Subscribe to `_session.OnFunctionError += HandleFunctionError` for error visibility
4. Start intro coroutine: `StartCoroutine(PlayIntroThenGoLive())`

**RegisterFunctions():**

Register three functions on `_session`:

a) `emote(animation_name)` -- FUNC-04 reference implementation:
```csharp
_session.RegisterFunction("emote",
    new FunctionDeclaration("emote",
        "Express an emotion or action visually. Call this to animate yourself.",
        new Dictionary<string, Schema>
        {
            { "animation_name", Schema.Enum(
                new[] { "idle", "wave", "think", "talk", "laugh", "shrug",
                        "fidgets", "nods_emphatically", "leans_forward",
                        "takes_deep_breath", "groans", "holds_up_hands",
                        "covers_face", "rolls_eyes", "stretches", "beams",
                        "puts_hand_over_heart" },
                "The animation to play") }
        }),
    HandleEmote);
```

b) `start_movie()` -- no parameters:
```csharp
_session.RegisterFunction("start_movie",
    new FunctionDeclaration("start_movie",
        "Cut away to show the movie scene. Use when telling a story that should be shown visually.",
        new Dictionary<string, Schema>()),
    HandleStartMovie);
```

c) `start_drawing()` -- no parameters:
```csharp
_session.RegisterFunction("start_drawing",
    new FunctionDeclaration("start_drawing",
        "Return to drawing on stream. Use when going back to creating art or when idle.",
        new Dictionary<string, Schema>()),
    HandleStartDrawing);
```

**Function handlers (all fire-and-forget, return null):**

`HandleEmote(FunctionCallContext ctx)`:
- `string animName = ctx.GetString("animation_name", "idle");`
- `_chatUI.LogSystemMessage($"[Emote: {animName}]");`
- Return null (fire-and-forget -- in a real game this would trigger an Animator)

`HandleStartMovie(FunctionCallContext ctx)`:
- `_chatUI.LogSystemMessage("[Movie mode activated]");`
- Return null

`HandleStartDrawing(FunctionCallContext ctx)`:
- `_chatUI.LogSystemMessage("[Drawing mode activated]");`
- Return null

**HandleFunctionError(string functionName, Exception ex):**
- `_chatUI.LogSystemMessage($"[Function error: {functionName} -- {ex.Message}]");`

**PlayIntroThenGoLive() coroutine:**
```csharp
private IEnumerator PlayIntroThenGoLive()
{
    _chatUI.SetStatus("Aya's intro playing...");
    _chatUI.LogSystemMessage("Welcome to Aya's Live Stream!");

    if (_introClip != null && _introAudioSource != null)
    {
        _introAudioSource.clip = _introClip;
        _introAudioSource.Play();
        yield return new WaitWhile(() => _introAudioSource.isPlaying);
    }
    else
    {
        // No intro clip -- brief pause then go live
        yield return new WaitForSeconds(1f);
    }

    _chatUI.LogSystemMessage("Going live...");
    _session.Connect();
    _liveMode = true;
}
```
Key points:
- Uses `_introAudioSource` (separate from AudioPlayback's AudioSource) to avoid OnAudioFilterRead conflict
- Null checks for intro clip/source allow testing without audio assets
- Connect() is called AFTER intro finishes -- functions were already registered in Start()

**Update() -- push-to-talk keyboard input:**
```csharp
void Update()
{
    if (!_liveMode) return;

    if (Keyboard.current != null)
    {
        if (Keyboard.current.spaceKey.wasPressedThisFrame)
            _session.StartListening();
        if (Keyboard.current.spaceKey.wasReleasedThisFrame)
            _session.StopListening();
    }
}
```
- Uses Input System (Keyboard.current) -- NOT legacy Input.GetKeyDown (project has activeInputHandler=1)
- Null check on Keyboard.current prevents NRE on platforms without keyboard (Pitfall 8)
- Only active when `_liveMode` is true (after intro finishes)

**HandleTurnComplete():**
```csharp
private void HandleTurnComplete()
{
    _exchangeCount++;
    if (_exchangeCount == WarmUpExchanges && !_goalActivated)
    {
        _goalActivated = true;
        _session.AddGoal(
            "life_story",
            "Steer the conversation toward talking about the life story behind your characters and what inspired you to start drawing them. Share personal anecdotes about your creative journey.",
            GoalPriority.Medium
        );
        _chatUI.LogSystemMessage("[Goal activated: Steer toward character life stories]");
    }
}
```
- Counts turn completions (each AI response = 1 exchange)
- After 3 exchanges, injects a Medium-priority conversational goal
- `_goalActivated` flag prevents double-injection

**OnDestroy():**
- Unsubscribe from `_session.OnTurnComplete` and `_session.OnFunctionError` if `_session != null`

**Using directives needed:**
- `using System`
- `using System.Collections`
- `using System.Collections.Generic`
- `using UnityEngine`
- `using UnityEngine.InputSystem`
- `using Firebase.AI`
- `using AIEmbodiment`

IMPORTANT: The `Schema.Enum()` method signature from the Firebase SDK is `Schema.Enum(IEnumerable<string> values, string description, bool nullable = false)`. Pass the string array and description only (nullable defaults to false).

IMPORTANT: The `FunctionDeclaration` constructor signature is `FunctionDeclaration(string name, string description, IDictionary<string, Schema> parameters, IEnumerable<string> optionalParameters = null)`. Pass name, description, and parameters dictionary only.
  </action>
  <verify>
Verify file exists. Check that all three RegisterFunction calls happen in a method called from Start() (before any Connect call). Verify FunctionDeclaration and Schema.Enum usage matches Firebase SDK signatures. Verify Keyboard.current.spaceKey usage with null check. Verify HandleTurnComplete counts exchanges and calls AddGoal at count 3. Verify namespace is AIEmbodiment.Samples.
  </verify>
  <done>
AyaSampleController.cs registers 3 function declarations (emote with Schema.Enum of 17 animations, start_movie, start_drawing) before Connect, plays intro coroutine on separate AudioSource, polls Keyboard.current.spaceKey for push-to-talk, counts exchanges and injects a Medium-priority conversational goal after 3 turns. All function handlers are fire-and-forget with system message logging.
  </done>
</task>

</tasks>

<verification>
1. Both C# files exist in Samples~/AyaLiveStream/
2. Both use namespace `AIEmbodiment.Samples`
3. AyaChatUI queries UXML elements by exact names from Plan 01 (chat-log, persona-name, speaking-indicator, status-label, ptt-button)
4. AyaSampleController registers 3 functions before Connect (RegisterFunctions called from Start, Connect called from coroutine after intro)
5. Schema.Enum used for emote animation_name parameter with 17 animation values from aya.json
6. Push-to-talk uses Keyboard.current (Input System) with null check, not legacy Input
7. Goal injection uses AddGoal with "life_story" ID at exchange count 3
8. Pre-recorded intro uses separate AudioSource from AudioPlayback
</verification>

<success_criteria>
- AyaChatUI.cs: Manages UIDocument, subscribes to all relevant PersonaSession events, displays streaming AI/user transcriptions with replacement semantics, toggles speaking glow, handles PTT button, logs system messages, auto-scrolls chat log
- AyaSampleController.cs: Registers emote (17 animations via Schema.Enum), start_movie, start_drawing before Connect. Plays intro on separate AudioSource. Polls spacebar via Input System. Injects "life_story" goal after 3 exchanges. Function handlers log to chat UI.
- Both scripts use only public API surface of PersonaSession -- no internal access or library modifications
</success_criteria>

<output>
After completion, create `.planning/phases/06-sample-scene-and-integration/06-02-SUMMARY.md`
</output>
