---
phase: 07-websocket-transport-and-audio-parsing
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - Packages/com.google.ai-embodiment/Runtime/GeminiLiveClient.cs
autonomous: true

must_haves:
  truths:
    - "Background receive loop accumulates multi-frame WebSocket messages and processes complete messages"
    - "setupComplete server message sets _setupComplete=true and enqueues Connected event"
    - "Audio inlineData is base64-decoded to bytes then converted to float[] (24kHz PCM) before enqueuing"
    - "outputTranscription text from serverContent is enqueued as GeminiEventType.OutputTranscription"
    - "inputTranscription text from serverContent is enqueued as GeminiEventType.InputTranscription"
    - "turnComplete boolean in serverContent enqueues GeminiEventType.TurnComplete"
    - "interrupted boolean in serverContent enqueues GeminiEventType.Interrupted"
    - "toolCall messages enqueue GeminiEventType.FunctionCall with name and args JSON"
    - "WebSocket close and errors are handled gracefully, enqueuing Disconnected or Error events"
  artifacts:
    - path: "Packages/com.google.ai-embodiment/Runtime/GeminiLiveClient.cs"
      provides: "ReceiveLoop and HandleJsonMessage methods replacing the stub"
      contains: "HandleJsonMessage"
  key_links:
    - from: "GeminiLiveClient.ReceiveLoop"
      to: "GeminiLiveClient.HandleJsonMessage"
      via: "UTF-8 decode of accumulated bytes"
      pattern: "HandleJsonMessage"
    - from: "GeminiLiveClient.HandleJsonMessage"
      to: "GeminiEvent.cs"
      via: "Enqueue(new GeminiEvent) for each parsed message type"
      pattern: "Enqueue.*GeminiEvent"
    - from: "HandleJsonMessage"
      to: "serverContent.modelTurn.parts[].inlineData"
      via: "JObject traversal for audio data"
      pattern: "inlineData.*data"
    - from: "HandleJsonMessage"
      to: "serverContent.outputTranscription/inputTranscription"
      via: "JObject traversal for transcription text"
      pattern: "outputTranscription|inputTranscription"
---

<objective>
Implement the receive loop and JSON message dispatch in GeminiLiveClient, completing the inbound data path.

Purpose: The client from Plan 01 can connect and send but cannot receive. This plan adds the background receive loop that reads WebSocket frames, accumulates multi-frame messages, parses JSON, and dispatches typed events (audio, transcriptions, turn lifecycle, function calls, errors) through the ConcurrentQueue. After this plan, GeminiLiveClient is a fully functional bidirectional WebSocket client.

Output: GeminiLiveClient.cs updated with ReceiveLoop and HandleJsonMessage methods replacing the stub from Plan 01.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-websocket-transport-and-audio-parsing/07-RESEARCH.md

# Plan 01 SUMMARY for understanding current state of GeminiLiveClient
@.planning/phases/07-websocket-transport-and-audio-parsing/07-01-SUMMARY.md

# Reference implementation (receive loop and JSON parsing)
@/home/cachy/workspaces/projects/persona/unity/Persona/Runtime/GeminiLiveClient.cs

# The file we are modifying
@Packages/com.google.ai-embodiment/Runtime/GeminiLiveClient.cs
@Packages/com.google.ai-embodiment/Runtime/GeminiEvent.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ReceiveLoop with multi-frame accumulation</name>
  <files>
    Packages/com.google.ai-embodiment/Runtime/GeminiLiveClient.cs
  </files>
  <action>
    Replace the stub ReceiveLoop method in GeminiLiveClient.cs with the full implementation. Add "using System.IO;" to the file's using statements if not already present.

    The ReceiveLoop method signature remains: private async Task ReceiveLoop(CancellationToken ct)

    Implementation (adapted from reference lines 467-533):

    1. Allocate a 64KB byte buffer: var buffer = new byte[64 * 1024];

    2. Wrap everything in try/catch:
       - Outer while loop: while (!ct.IsCancellationRequested && _ws?.State == WebSocketState.Open)
       - Inner frame accumulation: Create a new MemoryStream. Do/while loop calling _ws.ReceiveAsync(new ArraySegment<byte>(buffer), ct). If result.MessageType is Close, set _connected=false, _setupComplete=false, enqueue Disconnected event "Server closed connection", return. Write buffer bytes to MemoryStream. Continue until result.EndOfMessage is true.
       - After full message: Get bytes from MemoryStream. Check if JSON: bool isJson = bytes.Length > 0 && (bytes[0] == '{' || bytes[0] == '[');
       - If NOT JSON (defensive -- Gemini typically sends all as JSON, but handle raw binary): Enqueue as raw audio bytes (not converted to float[] -- this path may never trigger but is defensive).
       - If JSON: Decode as UTF-8 string, call HandleJsonMessage(text).

    3. Catch blocks:
       - OperationCanceledException: Normal shutdown, do nothing.
       - WebSocketException ex: Set _connected=false, _setupComplete=false, enqueue Error event with ex.Message.
       - Exception ex: Set _connected=false, _setupComplete=false, enqueue Error event with ex.ToString().

    IMPORTANT (Pitfall 3 from research): Gemini sends ALL messages (including JSON) as Binary WebSocket frames. The isJson check on the first byte is the correct approach -- do not rely on result.MessageType being Text.
    IMPORTANT (Pitfall 7 from research): MemoryStream allocation per message is acceptable for Phase 7. Do not optimize prematurely.
    IMPORTANT: Do NOT use async void. The method is async Task and is fire-and-forget via _ = ReceiveLoop() in ConnectAsync (already set up in Plan 01).
  </action>
  <verify>
    Grep for "ReceiveLoop" in GeminiLiveClient.cs -- should find the full implementation (not the stub).
    Grep for "EndOfMessage" in GeminiLiveClient.cs -- confirms multi-frame accumulation.
    Grep for "MemoryStream" in GeminiLiveClient.cs -- confirms message accumulation pattern.
    Grep for "HandleJsonMessage" in GeminiLiveClient.cs -- confirms JSON dispatch call.
  </verify>
  <done>
    ReceiveLoop reads WebSocket frames, accumulates multi-frame messages via MemoryStream, detects JSON via first-byte check, and dispatches to HandleJsonMessage. Handles Close frames, cancellation, WebSocket errors, and general exceptions gracefully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement HandleJsonMessage with full event dispatch</name>
  <files>
    Packages/com.google.ai-embodiment/Runtime/GeminiLiveClient.cs
  </files>
  <action>
    Add the HandleJsonMessage private method to GeminiLiveClient.cs. Also add "using Newtonsoft.Json;" to the using statements if not already present (needed for JsonException and Formatting).

    Method signature: private void HandleJsonMessage(string json)

    Implementation (adapted from reference lines 535-660, STRIPPED of all emote/animation/text-filtering logic):

    1. Parse JSON:
       JObject msg;
       try { msg = JObject.Parse(json); }
       catch (JsonException ex) { Enqueue error event "JSON parse error: " + ex.Message; return; }

    2. Check setupComplete:
       if (msg["setupComplete"] != null) -> set _setupComplete = true, enqueue Connected event "Session started", return.

    3. Check serverContent:
       var serverContent = msg["serverContent"] as JObject;
       if (serverContent != null):

       a. modelTurn.parts[] -- audio and text:
          var modelTurn = serverContent["modelTurn"] as JObject;
          if modelTurn != null, get parts as JArray, iterate:
          - inlineData: Get part["inlineData"] as JObject. If not null, get "data" as base64 string. Decode with Convert.FromBase64String. CONVERT 16-bit PCM bytes to float[] (this is the key difference from reference):
            int sampleCount = audioBytes.Length / 2;
            float[] floats = new float[sampleCount];
            for (int i = 0; i < sampleCount; i++)
            {
                short sample = (short)(audioBytes[i * 2] | (audioBytes[i * 2 + 1] << 8));
                floats[i] = sample / 32768f;
            }
            Enqueue GeminiEvent with Type=Audio, AudioData=floats, AudioSampleRate=24000.
          - text part: Get part["text"]?.ToString(). If not null/empty, enqueue as GeminiEventType.OutputTranscription. NOTE: In AUDIO modality, text inside modelTurn.parts is rare (the model primarily speaks). If it appears, treat it as output transcription text.

       b. turnComplete:
          var turnComplete = serverContent["turnComplete"];
          if (turnComplete != null && turnComplete.Value<bool>()) -> enqueue TurnComplete event.

       c. interrupted:
          var interrupted = serverContent["interrupted"];
          if (interrupted != null && interrupted.Value<bool>()) -> enqueue Interrupted event.

       d. outputTranscription (AI speech text -- AUD-04):
          var outputTranscription = serverContent["outputTranscription"] as JObject;
          if not null, get "text"?.ToString(). If not null/empty, enqueue OutputTranscription event.

       e. inputTranscription (user STT -- AUD-03):
          var inputTranscription = serverContent["inputTranscription"] as JObject;
          if not null, get "text"?.ToString(). If not null/empty, enqueue InputTranscription event.

    4. Check toolCall (top-level, alternative to serverContent):
       var toolCall = msg["toolCall"] as JObject;
       if toolCall != null:
       - Get functionCalls as JArray
       - Iterate: for each fc, extract name (fc["name"]?.ToString()), id (fc["id"]?.ToString()), args (fc["args"]?.ToString(Formatting.None) ?? "{}"). Enqueue FunctionCall event with FunctionName=name, FunctionArgsJson=args. NOTE: Do NOT auto-respond to tool calls here (that's Phase 10's job).

    5. Optionally handle toolCallCancellation (log it but do not act -- Phase 10 concern):
       var toolCallCancellation = msg["toolCallCancellation"] as JObject;
       If not null, enqueue an event or log. For now, just skip it -- Phase 10 will add cancellation handling.

    6. Optionally handle goAway:
       If msg["goAway"] != null, enqueue Error event "Server sent goAway -- session ending soon". This is informational per research Open Question 1.

    IMPORTANT (Pitfall 2 from research): Audio MUST be converted from 16-bit PCM bytes to float[] before enqueuing. AudioPlayback.EnqueueAudio expects float[]. The reference implementation returns raw bytes -- we convert here for cleaner downstream API.
    IMPORTANT: Do NOT add any emote parsing, text filtering, or animation logic. That is Phase 10.
    IMPORTANT: outputTranscription and inputTranscription are INSIDE serverContent, not top-level message fields (verified in research).
    IMPORTANT: The text part inside modelTurn.parts[] is distinct from outputTranscription. In AUDIO modality, outputTranscription is the primary text stream. modelTurn text parts are rare but should still be surfaced.
  </action>
  <verify>
    Grep for "setupComplete" in GeminiLiveClient.cs -- should find the handler setting _setupComplete=true.
    Grep for "inlineData" in GeminiLiveClient.cs -- should find audio decoding logic.
    Grep for "32768f" in GeminiLiveClient.cs -- confirms 16-bit PCM to float conversion.
    Grep for "outputTranscription" in GeminiLiveClient.cs -- should find the extraction inside serverContent.
    Grep for "inputTranscription" in GeminiLiveClient.cs -- should find the extraction inside serverContent.
    Grep for "turnComplete" in GeminiLiveClient.cs -- should find the boolean check and TurnComplete event.
    Grep for "interrupted" in GeminiLiveClient.cs -- should find the boolean check and Interrupted event.
    Grep for "toolCall" in GeminiLiveClient.cs -- should find function call parsing.
    Grep for "mediaChunks" in GeminiLiveClient.cs -- should find ZERO matches.
  </verify>
  <done>
    HandleJsonMessage parses all Gemini Live server message types: setupComplete (Connected event), serverContent with audio (base64 -> bytes -> float[]), outputTranscription (AUD-04), inputTranscription (AUD-03), turnComplete (AUD-05), interrupted (AUD-05), toolCall (FunctionCall event), and goAway (Error event). Audio is converted to float[] for direct AudioPlayback compatibility. No emote/animation logic present.
  </done>
</task>

</tasks>

<verification>
1. GeminiLiveClient.cs ReceiveLoop accumulates multi-frame messages via MemoryStream + EndOfMessage check
2. HandleJsonMessage dispatches all 7 server message types: setupComplete, audio, outputTranscription, inputTranscription, turnComplete, interrupted, toolCall
3. Audio is decoded from base64, converted from 16-bit PCM to float[], and enqueued with AudioSampleRate=24000
4. outputTranscription and inputTranscription are extracted from INSIDE serverContent (not top-level)
5. No emote, animation, or text-filtering logic exists in the file
6. No "mediaChunks" string appears anywhere in the file
7. OperationCanceledException is caught and suppressed (normal shutdown)
8. WebSocket close frames transition to Disconnected state
</verification>

<success_criteria>
GeminiLiveClient is now a complete bidirectional WebSocket client. It connects, sends the setup handshake, receives and parses all Gemini Live server message types, converts audio to float[] PCM, extracts both transcription streams, handles turn lifecycle events, and disconnects cleanly. All events flow through the ConcurrentQueue and are drained via ProcessEvents(). Phase 7 requirements WS-01 through WS-05 and AUD-01 through AUD-05 are fully implemented.
</success_criteria>

<output>
After completion, create `.planning/phases/07-websocket-transport-and-audio-parsing/07-02-SUMMARY.md`
</output>
