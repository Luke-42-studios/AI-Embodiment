---
phase: 11.1-queued-response-sample
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Assets/QueuedResponseSample/QueuedResponseSample.asmdef
  - Assets/QueuedResponseSample/QueuedResponseSample.asmdef.meta
  - Assets/QueuedResponseSample/QueuedResponseController.cs
  - Assets/QueuedResponseSample/QueuedResponseController.cs.meta
  - Assets/QueuedResponseSample/QueuedResponseUI.cs
  - Assets/QueuedResponseSample/QueuedResponseUI.cs.meta
  - Assets/QueuedResponseSample/UI/QueuedResponsePanel.uxml
  - Assets/QueuedResponseSample/UI/QueuedResponsePanel.uxml.meta
  - Assets/QueuedResponseSample/UI/QueuedResponsePanel.uss
  - Assets/QueuedResponseSample/UI/QueuedResponsePanel.uss.meta
  - Assets/QueuedResponseSample.meta
  - Assets/QueuedResponseSample/UI.meta
autonomous: true

must_haves:
  truths:
    - "QueuedResponseController implements a 5-state machine (Connecting, Idle, Recording, Reviewing, Playing) with correct keyboard-driven transitions"
    - "Audio from OnSyncPacket is buffered in a List<float[]> during Reviewing state and fed to AudioPlayback on Enter"
    - "Audio arriving during Playing state is fed directly to AudioPlayback for live streaming"
    - "User transcript displays in real-time during Recording via OnInputTranscription"
    - "AI response transcript accumulates via OnOutputTranscription and displays during Playing state"
    - "Escape during Reviewing discards buffers and returns to Idle"
    - "Playback completion detected via _turnComplete && !_audioPlayback.IsPlaying"
  artifacts:
    - path: "Assets/QueuedResponseSample/QueuedResponseSample.asmdef"
      provides: "Assembly definition referencing ai-embodiment and InputSystem"
      contains: "com.google.ai-embodiment.samples.queuedresponse"
    - path: "Assets/QueuedResponseSample/QueuedResponseController.cs"
      provides: "State machine controller with audio buffering"
      min_lines: 120
    - path: "Assets/QueuedResponseSample/QueuedResponseUI.cs"
      provides: "UI Toolkit bindings for queued response layout"
      min_lines: 50
    - path: "Assets/QueuedResponseSample/UI/QueuedResponsePanel.uxml"
      provides: "UI layout with user-transcript, ai-response, status-label, key-hints, state-indicator elements"
    - path: "Assets/QueuedResponseSample/UI/QueuedResponsePanel.uss"
      provides: "Dark theme styling matching Aya sample conventions"
  key_links:
    - from: "QueuedResponseController.cs"
      to: "PersonaSession"
      via: "event subscriptions in Start()"
      pattern: "_session\\.On(InputTranscription|OutputTranscription|SyncPacket|TurnComplete|StateChanged)"
    - from: "QueuedResponseController.cs"
      to: "AudioPlayback"
      via: "EnqueueAudio in EnterPlaying and HandleSyncPacket"
      pattern: "_audioPlayback\\.EnqueueAudio"
    - from: "QueuedResponseUI.cs"
      to: "QueuedResponsePanel.uxml"
      via: "Q<> queries on UIDocument root"
      pattern: "root\\.Q<.+>\\(\"(user-transcript|ai-response|status-label|key-hints)\"\\)"
---

<objective>
Create the QueuedResponseSample project structure, UI assets, and core C# scripts (QueuedResponseController and QueuedResponseUI) implementing the push-to-talk transcript approval UX.

Purpose: This is the complete implementation of the queued response sample -- the state machine controller that buffers AI audio until user approval, the UI controller that binds to UI Toolkit elements, and all supporting project files (asmdef, UXML, USS, .meta files).

Output: A compilable Assets/QueuedResponseSample/ directory with all C# code, UI assets, and Unity metadata files. The scene file is NOT created here (that requires Unity Editor), but all scripts and assets referenced by the scene are ready.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11.1-queued-response-sample/11.1-RESEARCH.md

@Assets/AyaLiveStream/AyaSampleController.cs
@Assets/AyaLiveStream/AyaChatUI.cs
@Assets/AyaLiveStream/AyaLiveStream.asmdef
@Assets/AyaLiveStream/UI/AyaPanel.uxml
@Assets/AyaLiveStream/UI/AyaPanel.uss
@Packages/com.google.ai-embodiment/Runtime/AudioPlayback.cs
@Packages/com.google.ai-embodiment/Runtime/SyncPacket.cs
@Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project structure and UI assets</name>
  <files>
    Assets/QueuedResponseSample.meta
    Assets/QueuedResponseSample/UI.meta
    Assets/QueuedResponseSample/QueuedResponseSample.asmdef
    Assets/QueuedResponseSample/QueuedResponseSample.asmdef.meta
    Assets/QueuedResponseSample/UI/QueuedResponsePanel.uxml
    Assets/QueuedResponseSample/UI/QueuedResponsePanel.uxml.meta
    Assets/QueuedResponseSample/UI/QueuedResponsePanel.uss
    Assets/QueuedResponseSample/UI/QueuedResponsePanel.uss.meta
  </files>
  <action>
    Create the QueuedResponseSample directory structure with all .meta files following the existing 2-line .meta convention (fileFormatVersion: 2 + guid). Generate unique GUIDs for each .meta file (use lowercase hex, 32 chars). Folder .meta files use `folderAsset: yes` on the third line plus `DefaultImporter` block (match the existing Assets/AyaLiveStream.meta pattern if it exists, or use the pattern from 11-01 decisions).

    **Assembly definition (QueuedResponseSample.asmdef):**
    Follow the exact pattern from AyaLiveStream.asmdef:
    ```json
    {
        "name": "com.google.ai-embodiment.samples.queuedresponse",
        "rootNamespace": "AIEmbodiment.Samples",
        "references": [
            "com.google.ai-embodiment",
            "Unity.InputSystem"
        ],
        "includePlatforms": [],
        "excludePlatforms": [],
        "allowUnsafeCode": false,
        "overrideReferences": false,
        "precompiledReferences": [],
        "autoReferenced": true,
        "defineConstraints": [],
        "versionDefines": [],
        "noEngineReferences": false
    }
    ```

    **UXML layout (QueuedResponsePanel.uxml):**
    Two-panel layout (NOT a scrolling chat log). Elements required:
    - `root-container` (class="root") -- full-screen dark container
    - `header` with `state-indicator` (VisualElement, class="indicator") and `status-label` (Label)
    - `transcript-area` with section header "You:" and `user-transcript` (Label, class="transcript user-text") -- white-space: normal for wrapping
    - `response-area` with section header "AI:" and `ai-response` (Label, class="transcript ai-text") -- white-space: normal for wrapping
    - `footer` with `key-hints` (Label, class="key-hints")

    **USS styles (QueuedResponsePanel.uss):**
    Follow the AyaPanel.uss color scheme and conventions:
    - Same dark background (rgb(24, 24, 32))
    - Same indicator/indicator--speaking pattern
    - transcript-area and response-area: dark inner panels (rgb(16, 16, 24)), border-radius 8px, padding 8px, flex-grow 1
    - user-text: rgb(170, 220, 255) (same as msg-user in Aya)
    - ai-text: rgb(200, 170, 255) (same as msg-aya in Aya)
    - section-header: smaller font, muted color
    - key-hints: centered, muted color, bottom of screen
    - status-label: same as Aya .status class
    - Add `.response-ready` class on state-indicator: pulsing green glow (use transition or border-color change)
    - Add `.recording` class on state-indicator: red glow for recording state
  </action>
  <verify>
    All files exist: `ls -R Assets/QueuedResponseSample/`
    asmdef is valid JSON: `python3 -c "import json; json.load(open('Assets/QueuedResponseSample/QueuedResponseSample.asmdef'))"`
    UXML has required elements: grep for user-transcript, ai-response, status-label, key-hints, state-indicator in the UXML file
  </verify>
  <done>
    Assets/QueuedResponseSample/ directory exists with asmdef, UXML, USS, and all .meta files. UXML contains all 5 required UI element names. USS has dark theme matching Aya conventions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create QueuedResponseController with state machine and audio buffering</name>
  <files>
    Assets/QueuedResponseSample/QueuedResponseController.cs
    Assets/QueuedResponseSample/QueuedResponseController.cs.meta
  </files>
  <action>
    Create QueuedResponseController.cs following the research skeleton closely but with these refinements:

    **Namespace:** `AIEmbodiment.Samples`
    **Using statements:** System.Collections.Generic, System.Text, UnityEngine, UnityEngine.InputSystem, AIEmbodiment

    **Enum QueuedState:** Connecting, Idle, Recording, Reviewing, Playing (defined in this file, above the class)

    **SerializeField references:**
    - `_session` (PersonaSession)
    - `_audioPlayback` (AudioPlayback) -- separate from PersonaSession, controller manages it
    - `_ui` (QueuedResponseUI)

    **Private state:**
    - `_state` (QueuedState) initialized to Connecting
    - `_audioBuffer` (List<float[]>) -- readonly, new in declaration
    - `_aiTranscript` (StringBuilder) -- readonly, new in declaration
    - `_turnComplete` (bool)
    - `_playbackInitialized` (bool)

    **Start():**
    Subscribe to PersonaSession events: OnStateChanged, OnInputTranscription, OnOutputTranscription, OnSyncPacket, OnTurnComplete. Then call `_session.Connect()`.

    **Update():**
    Null-check Keyboard.current. Switch on _state:
    - Idle: spaceKey.wasPressedThisFrame -> EnterRecording()
    - Recording: spaceKey.wasReleasedThisFrame -> EnterReviewing()
    - Reviewing: enterKey.wasPressedThisFrame -> EnterPlaying(), escapeKey.wasPressedThisFrame -> DiscardAndReturnToIdle()
    - Playing: if `_turnComplete && !_audioPlayback.IsPlaying` -> EnterIdle()

    **EnterRecording():**
    Set state, clear _audioBuffer, clear _aiTranscript, set _turnComplete false, call `_audioPlayback?.ClearBuffer()`, call `_session.StartListening()`, call `_ui.SetState(QueuedState.Recording)`, call `_ui.ClearTranscripts()`.

    **EnterReviewing():**
    Set state, call `_session.StopListening()`, call `_ui.SetState(QueuedState.Reviewing)`.

    **EnterPlaying():**
    Set state, call EnsurePlaybackInitialized(). Feed ALL buffered audio to AudioPlayback via foreach loop, then clear _audioBuffer. Call `_ui.SetState(QueuedState.Playing)`. Call `_ui.SetAITranscript(_aiTranscript.ToString())`.

    **EnterIdle():**
    Set state, call `_ui.SetState(QueuedState.Idle)`.

    **DiscardAndReturnToIdle():**
    Clear _audioBuffer, clear _aiTranscript, set _turnComplete false, call `_audioPlayback?.ClearBuffer()`, set state to Idle, call `_ui.SetState(QueuedState.Idle)`, call `_ui.ClearTranscripts()`.

    **EnsurePlaybackInitialized():**
    Guard: if _playbackInitialized or _audioPlayback null, return. Call `_audioPlayback.Initialize()`, set _playbackInitialized true.

    **HandleSyncPacket(SyncPacket packet):**
    If packet.Type != SyncPacketType.TextAudio, return.
    If _state == Reviewing: buffer audio (null-check packet.Audio and length > 0 before adding).
    If _state == Playing: feed directly to AudioPlayback via EnsurePlaybackInitialized() + EnqueueAudio (null-check audio).

    **HandleOutputTranscription(string text):**
    If _state == Reviewing or Playing: append text to _aiTranscript. If _state == Playing: also call `_ui.SetAITranscript(_aiTranscript.ToString())`.

    **HandleInputTranscription(string text):**
    If _state == Recording or Reviewing: call `_ui.SetUserTranscript(text)`. (Replacement semantics -- Gemini sends full transcript each time.)

    **HandleTurnComplete():**
    Set _turnComplete = true. If _state == Reviewing: call `_ui.ShowReadyIndicator()`.

    **HandleStateChanged(SessionState state):**
    If Connected: set _state to Idle, call `_ui.SetState(QueuedState.Idle)`.
    If Error: call `_ui.SetStatus("Connection error. Restart the scene.")`.

    **OnDestroy():**
    Unsubscribe all events from _session (null-check _session first). Mirror the AyaSampleController.OnDestroy pattern.

    **IMPORTANT:** Do NOT subscribe to OnAISpeakingStarted/Stopped -- the research explains why OnAISpeakingStopped fires too early for the queued pattern. Use `_turnComplete && !_audioPlayback.IsPlaying` instead.

    **XML doc comments:** Add `<summary>` on the class and each public/SerializeField. Follow AyaSampleController's style (concise, single sentence).
  </action>
  <verify>
    File exists and contains all state transitions: grep for EnterRecording, EnterReviewing, EnterPlaying, EnterIdle, DiscardAndReturnToIdle.
    Grep for key patterns: `_audioBuffer.Add`, `_audioPlayback.EnqueueAudio`, `_session.Connect()`, `_session.StartListening()`, `_session.StopListening()`.
    Grep for correct playback completion check: `_turnComplete && !_audioPlayback.IsPlaying`.
    Ensure NO subscription to OnAISpeakingStarted or OnAISpeakingStopped.
  </verify>
  <done>
    QueuedResponseController.cs compiles with complete 5-state machine, correct audio buffering (buffer during Reviewing, feed during Playing, live-stream if AI still going), keyboard input handling, and proper event subscriptions/unsubscriptions.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create QueuedResponseUI with state-driven display</name>
  <files>
    Assets/QueuedResponseSample/QueuedResponseUI.cs
    Assets/QueuedResponseSample/QueuedResponseUI.cs.meta
  </files>
  <action>
    Create QueuedResponseUI.cs following the research skeleton with these specifics:

    **Namespace:** `AIEmbodiment.Samples`
    **Using statements:** UnityEngine, UnityEngine.UIElements

    **SerializeField:**
    - `_uiDocument` (UIDocument)

    **Private UI references (set in OnEnable):**
    - `_userTranscript` (Label) -- Q<Label>("user-transcript")
    - `_aiResponse` (Label) -- Q<Label>("ai-response")
    - `_statusLabel` (Label) -- Q<Label>("status-label")
    - `_keyHints` (Label) -- Q<Label>("key-hints")
    - `_stateIndicator` (VisualElement) -- Q("state-indicator")

    **OnEnable():**
    Get root from _uiDocument.rootVisualElement. Query all 5 elements.

    **Public methods:**

    `SetState(QueuedState state)`:
    Update _statusLabel.text with state-specific messages:
    - Connecting: "Connecting to AI..."
    - Idle: "Ready"
    - Recording: "Recording... Release SPACE when done."
    - Reviewing: "Review your message."
    - Playing: "AI is responding..."

    Update _keyHints.text with state-specific hints:
    - Connecting: ""
    - Idle: "[SPACE] Record"
    - Recording: "[SPACE] Release to stop"
    - Reviewing: "[ENTER] Send  |  [ESC] Cancel"
    - Playing: ""

    Update _stateIndicator classes:
    - Remove all state classes first: RemoveFromClassList("recording"), RemoveFromClassList("response-ready")
    - Recording: AddToClassList("recording")
    - Playing: AddToClassList("indicator--speaking") (reuse existing green glow)
    - All others: just the removes (indicator goes grey)

    `SetUserTranscript(string text)`:
    Set _userTranscript.text = text.

    `SetAITranscript(string text)`:
    Set _aiResponse.text = text.

    `ClearTranscripts()`:
    Set _userTranscript.text = "" and _aiResponse.text = "".

    `ShowReadyIndicator()`:
    AddToClassList("response-ready") on _stateIndicator.

    `SetStatus(string text)`:
    Set _statusLabel.text = text. (Escape hatch for direct status messages like errors.)

    **XML doc comments:** Add `<summary>` on the class and each public method. Concise.
  </action>
  <verify>
    File exists and contains all public methods: grep for SetState, SetUserTranscript, SetAITranscript, ClearTranscripts, ShowReadyIndicator, SetStatus.
    Grep for all UI element queries: user-transcript, ai-response, status-label, key-hints, state-indicator.
    Verify QueuedState enum is referenced (imported from QueuedResponseController or shared namespace).
  </verify>
  <done>
    QueuedResponseUI.cs compiles with state-driven status/hint text, transcript display methods, ready indicator, and proper UI Toolkit element binding matching the UXML element names.
  </done>
</task>

</tasks>

<verification>
All files exist in Assets/QueuedResponseSample/:
- QueuedResponseSample.asmdef (+ .meta)
- QueuedResponseController.cs (+ .meta)
- QueuedResponseUI.cs (+ .meta)
- UI/QueuedResponsePanel.uxml (+ .meta)
- UI/QueuedResponsePanel.uss (+ .meta)
- Folder .meta files for QueuedResponseSample/ and UI/

asmdef references both com.google.ai-embodiment and Unity.InputSystem.
UXML contains all 5 required element names (user-transcript, ai-response, status-label, key-hints, state-indicator).
Controller subscribes to OnSyncPacket, OnInputTranscription, OnOutputTranscription, OnTurnComplete, OnStateChanged.
Controller does NOT subscribe to OnAISpeakingStarted or OnAISpeakingStopped.
Controller checks playback completion via _turnComplete && !_audioPlayback.IsPlaying.
</verification>

<success_criteria>
A complete Assets/QueuedResponseSample/ directory exists with:
1. Valid asmdef referencing the runtime package and InputSystem
2. QueuedResponseController with 5-state machine, audio buffering, and keyboard input
3. QueuedResponseUI with state-driven display methods matching UXML element names
4. UXML layout with two-panel transcript design (user + AI areas)
5. USS styles matching the Aya sample dark theme conventions
6. All .meta files with unique GUIDs following the 2-line format convention
</success_criteria>

<output>
After completion, create `.planning/phases/11.1-queued-response-sample/11.1-01-SUMMARY.md`
</output>
