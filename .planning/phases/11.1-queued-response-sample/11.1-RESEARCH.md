# Phase 11.1: Queued Response Sample - Research

**Researched:** 2026-02-13
**Domain:** Unity UI state machine, audio buffering, PersonaSession event-driven architecture
**Confidence:** HIGH

## Summary

This phase creates a new sample scene demonstrating a "queued response" UX: the user holds spacebar to record, sees their transcript, reviews and approves with Enter, and the AI response (which was pre-fetched in the background while the user reviewed) plays back immediately. The entire phase is internal to this codebase -- no new external libraries are needed.

The core architectural challenge is **decoupling audio arrival from audio playback**. In the existing AyaLiveStream sample, PersonaSession automatically routes audio to AudioPlayback as it arrives. For the queued pattern, audio must accumulate silently in a buffer until the user explicitly approves, then drain to AudioPlayback all at once. The PersonaSession API already supports this via a null `_audioPlayback` reference combined with `OnSyncPacket` events that carry audio data.

The UX flow is a five-state machine: Idle, Recording, Reviewing, Playing, WaitingForAI. The controller manages transitions between these states via keyboard input (spacebar, Enter, Escape) and PersonaSession events (OnInputTranscription, OnSyncPacket, OnTurnComplete, OnAISpeakingStopped).

**Primary recommendation:** Create a new sample scene with a `QueuedResponseController` and `QueuedResponseUI` that use PersonaSession with `_audioPlayback` left null (unassigned), collect audio from `OnSyncPacket` events into a `List<float[]>` buffer, and manually feed a separately-managed AudioPlayback when the user presses Enter to approve.

## Standard Stack

No new libraries or dependencies. This phase uses only existing project components:

### Core Components (all exist, no source changes needed)
| Component | Location | Purpose | Key for Phase 11.1 |
|-----------|----------|---------|---------------------|
| PersonaSession | Runtime/PersonaSession.cs | Session lifecycle, event routing | `_audioPlayback` left null; audio arrives via OnSyncPacket |
| AudioCapture | Runtime/AudioCapture.cs | Microphone capture at 16kHz | Used for push-to-talk recording |
| AudioPlayback | Runtime/AudioPlayback.cs | Ring buffer streaming playback | Managed by controller, not PersonaSession |
| AudioRingBuffer | Runtime/AudioRingBuffer.cs | Lock-free SPSC circular buffer | Used internally by AudioPlayback |
| PacketAssembler | Runtime/PacketAssembler.cs | Correlates text+audio into SyncPackets | Still gets audio via AddAudio even when _audioPlayback is null |
| SyncPacket | Runtime/SyncPacket.cs | Correlated text/audio/function data | Controller reads `.Audio` float[] from these |
| GeminiLiveClient | Runtime/GeminiLiveClient.cs | WebSocket transport | Unchanged |
| PersonaConfig | ScriptableObject | Persona configuration | Reuse existing AyaPersonaConfig or create new one |

### New Components (to be created)
| Component | Purpose | Pattern |
|-----------|---------|---------|
| QueuedResponseController | State machine orchestrating the queued UX flow | MonoBehaviour, similar structure to AyaSampleController |
| QueuedResponseUI | UI Toolkit controller for the queued response layout | MonoBehaviour, similar structure to AyaChatUI |

### UI Assets (to be created)
| Asset | Purpose |
|-------|---------|
| QueuedResponsePanel.uxml | UI Toolkit layout for the queued response scene |
| QueuedResponsePanel.uss | Styles for the queued response scene |

## Architecture Patterns

### The Null-AudioPlayback Decoupling Pattern

The critical architectural insight: PersonaSession's `_audioPlayback` field is optional (`[SerializeField]` with null-conditional access throughout). When null:

1. `HandleAudioEvent` still fires -- audio goes to `_packetAssembler?.AddAudio(ev.AudioData)` but NOT to playback
2. `OnSyncPacket` events still carry `float[] Audio` data from PacketAssembler
3. `OnAISpeakingStarted`/`OnAISpeakingStopped` still fire (tracked via `_aiSpeaking` flag)
4. `OnOutputTranscription`/`OnInputTranscription` still fire normally
5. `OnTurnComplete` still fires when Gemini signals turn end

The controller subscribes to `OnSyncPacket`, buffers the audio arrays, and feeds them to a separately-wired AudioPlayback when the user approves.

**Verification from source code (PersonaSession.cs line 484):**
```csharp
// In HandleAudioEvent:
_audioPlayback?.EnqueueAudio(ev.AudioData);  // skipped when null

// But PacketAssembler STILL gets audio (line 494):
_packetAssembler?.AddAudio(ev.AudioData);
```

**Verification from source code (PersonaSession.cs lines 486-491):**
```csharp
// AI speaking state tracked regardless of _audioPlayback:
if (!_aiSpeaking)
{
    _aiSpeaking = true;
    OnAISpeakingStarted?.Invoke();
}
```

### State Machine Design

```
                 +---------+
                 |  IDLE   |  <-- Initial state after Connect()
                 +----+----+
                      |
              spacebar pressed
                      |
                 +----v----+
                 |RECORDING|  <-- Mic active, audio streaming to Gemini
                 +----+----+
                      |
              spacebar released
              (StopListening + audioStreamEnd)
                      |
                 +----v------+
                 | REVIEWING |  <-- Transcript shown, AI response streaming in background
                 +--+----+---+     Audio buffered in List<float[]>, NOT playing
                    |    |
            Enter   |    | Escape
            pressed |    | pressed
                    |    |
             +------v-+  +----v------+
             |PLAYING |  | DISCARDED |  --> back to IDLE (clear buffers)
             +----+---+  +-----------+
                  |
          OnAISpeakingStopped + buffer drained
                  |
             +----v----+
             |  IDLE   |  <-- Ready for next turn
             +---------+
```

**State descriptions:**

| State | What's Happening | User Input | AI Events |
|-------|-----------------|------------|-----------|
| **Idle** | Connected, waiting for user | Spacebar -> Recording | None expected |
| **Recording** | Mic active, audio streaming | Release spacebar -> Reviewing | OnInputTranscription (real-time) |
| **Reviewing** | User sees transcript, AI response streams in background | Enter -> Playing, Escape -> Idle | OnSyncPacket (buffered), OnOutputTranscription (buffered text), OnTurnComplete (may arrive) |
| **Playing** | Buffered audio draining to AudioPlayback | None (wait for completion) | OnAISpeakingStopped when buffer drains |
| **Idle** (again) | Loop back to start | Spacebar -> Recording | None |

### Audio Buffering Strategy

During the Reviewing state, collect audio from SyncPackets:

```csharp
private readonly List<float[]> _audioQueue = new List<float[]>();
private bool _turnComplete = false;

// Called from OnSyncPacket handler during Reviewing state:
private void BufferAudio(SyncPacket packet)
{
    if (packet.Type == SyncPacketType.TextAudio && packet.Audio != null)
    {
        _audioQueue.Add(packet.Audio);
    }
}

// Called when user presses Enter:
private void ApproveAndPlay()
{
    _audioPlayback.Initialize(); // idempotent if already initialized
    foreach (var chunk in _audioQueue)
    {
        _audioPlayback.EnqueueAudio(chunk);
    }
    _audioQueue.Clear();
    _state = QueuedState.Playing;
}
```

**Important:** AudioPlayback.Initialize() creates a dummy AudioClip and starts the AudioSource loop. It should only be called once (it is not idempotent -- it replaces the clip each time). The controller should track initialization state.

### Handling Edge Cases in the State Machine

**Case 1: AI response completes before user approves (fast AI)**
- OnTurnComplete arrives during Reviewing state
- Controller sets `_turnComplete = true` flag
- All audio is in the buffer
- When user presses Enter: play all buffered audio, entire response plays immediately
- This is the "happy path" for the near-zero latency requirement

**Case 2: User approves before AI response completes (slow AI)**
- User presses Enter during Reviewing state while AI is still streaming
- Controller transitions to Playing, feeds buffered audio to AudioPlayback
- Continues buffering new SyncPacket audio and feeding directly to AudioPlayback
- Effectively becomes "live streaming" mode after the initial buffer drain
- Still feels responsive because the buffered audio starts immediately

**Case 3: User discards (Escape during Reviewing)**
- Clear audio buffer
- Reset transcript display
- Transition back to Idle
- The AI response that was streaming is now "orphaned" -- it will complete and fire OnTurnComplete
- Controller ignores events from the orphaned turn (track turn ID)

**Case 4: User starts new recording before AI turn completes**
- If in Idle and user presses spacebar, StartListening interrupts the previous session context
- PersonaSession handles this via the Gemini Interrupted event path
- Audio buffer should be cleared on new recording start

### Recommended Project Structure

```
Assets/
  QueuedResponseSample/
    QueuedResponseController.cs   # State machine, audio buffering
    QueuedResponseUI.cs           # UI Toolkit bindings
    QueuedResponseSample.asmdef   # Assembly definition
    UI/
      QueuedResponsePanel.uxml    # Layout
      QueuedResponsePanel.uss     # Styles
  Scenes/
    QueuedResponseScene.unity     # New scene file

Packages/com.google.ai-embodiment/
  Samples~/
    QueuedResponseSample/         # Canonical copy for package import
      (same files as above minus scene)
```

### UI Layout Design

The UI needs different elements than the AyaLiveStream chat log:

```
+------------------------------------------+
| [indicator] AI Persona Name              |  <-- header (same as Aya)
+------------------------------------------+
|                                          |
|  [Transcript Area]                       |  <-- Shows user transcript during
|  "Your message will appear here..."      |      Recording and Reviewing states
|                                          |
+------------------------------------------+
|                                          |
|  [AI Response Area]                      |  <-- Shows AI response text during
|  "AI response will appear here..."       |      Playing state
|                                          |
+------------------------------------------+
| Status: Hold SPACE to talk               |  <-- State-dependent status
| [SPACE: Record] [ENTER: Send] [ESC: Cancel] |  <-- Key hints
+------------------------------------------+
```

Key UI elements:
- `user-transcript` (Label) -- Shows the user's speech transcript in real-time
- `ai-response` (Label) -- Shows the AI's response transcript during playback
- `status-label` (Label) -- State-dependent status text
- `key-hints` (Label) -- Shows available actions for current state
- `speaking-indicator` (VisualElement) -- Same glow indicator as Aya sample
- `state-indicator` (Label) -- Shows current state (Recording, Reviewing, Playing)

### Recommended Scene Hierarchy

```
QueuedResponseScene.unity
  Main Camera              (Camera, AudioListener)
  Directional Light        (Light)
  UIDocument               (UIDocument, QueuedResponseUI)
  AISession                (PersonaSession, AudioCapture)      <-- NO AudioPlayback here
  AudioPlayer              (AudioPlayback, AudioSource)        <-- Separate GO, controller manages
  QueuedResponseController (QueuedResponseController)
```

Key wiring differences from AyaLiveStream:
- PersonaSession `_audioPlayback` is **null** (not wired)
- AudioPlayback lives on a separate GameObject
- QueuedResponseController holds references to both PersonaSession and AudioPlayback
- No intro audio (simpler flow)

### Transcript Accumulation Pattern

OnInputTranscription fires with the FULL accumulated user transcript (not incremental deltas). The Gemini server returns the complete transcription each time, replacing the previous value. This simplifies the UI -- just set `_userTranscript.text = text` on each event.

OnOutputTranscription fires incrementally during the AI's turn. The controller should accumulate these into a StringBuilder and display during the Playing state.

**Verification from GeminiLiveClient.cs (lines 458-471):**
```csharp
// inputTranscription arrives as serverContent.inputTranscription.text
var inputTranscription = serverContent["inputTranscription"] as JObject;
if (inputTranscription != null)
{
    var t = inputTranscription["text"]?.ToString();
    // ... fires as GeminiEventType.InputTranscription
}
```

The Gemini Live API sends `inputTranscription` messages as the user speaks, with each message containing the full transcript so far (not deltas). This is confirmed by the existing AyaChatUI behavior which sets `_currentUserMessage.text = $"You: {text}"` (replacing, not appending).

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Audio ring buffer | Custom circular buffer | AudioPlayback + AudioRingBuffer | Already handles resampling, watermark buffering, thread safety |
| PCM audio conversion | Manual byte-to-float conversion | GeminiLiveClient does this internally | Audio arrives as float[] in GeminiEvent/SyncPacket |
| WebSocket management | Manual WS connection | PersonaSession.Connect/Disconnect | Complete lifecycle management |
| Input transcription | Manual speech-to-text | Gemini's inputAudioTranscription | Already configured in setup handshake |
| Output transcription | Manual STT on AI audio | Gemini's outputAudioTranscription | Already configured in setup handshake |
| Sentence splitting | Custom text chunking | PacketAssembler (already running) | Handles sentence boundaries, timeout fallbacks |
| Thread safety for events | Manual thread dispatch | GeminiLiveClient ConcurrentQueue + ProcessEvents | All events arrive on main thread via Update() |

**Key insight:** The entire infrastructure for audio capture, WebSocket communication, transcription, and audio decoding already exists. This phase is purely a new UX controller and UI on top of the existing event-driven API.

## Common Pitfalls

### Pitfall 1: AudioPlayback.Initialize() called multiple times
**What goes wrong:** Each call creates a new dummy AudioClip and calls AudioSource.Play(), potentially causing audio glitches or resource leaks.
**Why it happens:** The controller might call Initialize() on each approval if not tracking init state.
**How to avoid:** Call Initialize() once in Start() or on first approval, track with a `_playbackInitialized` boolean. AudioPlayback has no built-in idempotency guard.
**Warning signs:** Audio clicks or pops on second/third approval cycle.

### Pitfall 2: Audio buffered but AudioPlayback not initialized
**What goes wrong:** EnqueueAudio() is called on AudioPlayback before Initialize(). The ring buffer is null, so the write is silently dropped (null-conditional `_ringBuffer?.Write()`... actually no -- EnqueueAudio does `if (_ringBuffer == null) return;`).
**Why it happens:** Controller feeds buffered audio before calling Initialize().
**How to avoid:** Always call Initialize() before the first EnqueueAudio(). Initialize AudioPlayback eagerly in Start().
**Warning signs:** No audio plays despite buffer having data. No errors logged.

### Pitfall 3: Stale audio in buffer from previous turn
**What goes wrong:** User discards (Escape), starts a new recording, approves -- hears audio from the previous AI response mixed with the new one.
**Why it happens:** Audio buffer not cleared on state transitions.
**How to avoid:** Clear `_audioQueue` and call `_audioPlayback.ClearBuffer()` on every transition back to Idle or to Recording.
**Warning signs:** AI says something unrelated to the current transcript.

### Pitfall 4: OnAISpeakingStopped never fires when _audioPlayback is null
**What goes wrong:** PersonaSession tracks `_aiSpeaking` and fires OnAISpeakingStopped in `HandleTurnCompleteEvent()`. This DOES fire even when `_audioPlayback` is null. However, it fires immediately on TurnComplete, not when audio finishes playing (since PersonaSession doesn't know about the queued audio).
**Why it happens:** In the normal flow, OnAISpeakingStopped means "AI finished producing audio." In the queued flow, the user may still be hearing audio long after this event fires.
**How to avoid:** The controller should NOT use OnAISpeakingStopped to detect "playback complete." Instead, poll `_audioPlayback.IsPlaying` or use a coroutine/timer to detect when the ring buffer has drained after the last EnqueueAudio call.
**Warning signs:** Controller transitions to Idle while audio is still playing.

### Pitfall 5: Entering Playing state before any audio has been buffered
**What goes wrong:** User speaks very briefly, presses Enter immediately. The AI hasn't responded yet -- no SyncPackets with audio have arrived. AudioPlayback has nothing to play.
**Why it happens:** The AI needs time to process the audio and generate a response. Very fast Enter presses after short recordings may beat the server.
**How to avoid:** When entering Playing state, if no audio is buffered, show "Waiting for AI..." status and begin feeding audio as it arrives (live streaming mode). Alternatively, disable the Enter key until at least one audio chunk has arrived.
**Warning signs:** Silent playback with no audio, or premature return to Idle.

### Pitfall 6: SyncPacket audio may be null even for TextAudio type
**What goes wrong:** In certain timing scenarios, a SyncPacket may have text but null audio (e.g., if transcription arrives before audio data).
**Why it happens:** PacketAssembler emits packets at sentence boundaries. If text reaches a boundary before audio accumulates, the packet has text but empty/null audio.
**How to avoid:** Always null-check `packet.Audio` before adding to the buffer. This is already shown in the buffering pattern above.
**Warning signs:** NullReferenceException when iterating audio buffer.

### Pitfall 7: Multiple OnInputTranscription events during recording
**What goes wrong:** The transcript label flickers or appears to reset.
**Why it happens:** Gemini sends multiple inputTranscription events as the user speaks. Each contains the FULL transcript so far (replacement semantics, not delta).
**How to avoid:** Simply overwrite the label text each time: `_userTranscript.text = text`. This is the correct pattern -- each event contains the complete transcript.
**Warning signs:** None if handled correctly; flicker if trying to append.

### Pitfall 8: AudioPlayback ring buffer overflow on long queued responses
**What goes wrong:** If the AI generates a very long response (many seconds of audio) while the user sits in Reviewing state, all that audio accumulates in the List<float[]> buffer. When the user approves, all audio is dumped into the ring buffer at once, potentially overflowing it.
**Why it happens:** AudioRingBuffer has a fixed capacity of 30 seconds (24000 * 30 = 720,000 samples). If the AI response exceeds 30 seconds of audio, the early audio gets silently clamped/dropped.
**How to avoid:** Feed the buffer gradually rather than all at once. Use a coroutine that feeds chunks each frame, or feed a limited amount per frame. Alternatively, since 30 seconds is quite long for a single response, this may not be a practical issue.
**Warning signs:** Audio truncation on very long AI responses.

## Code Examples

### QueuedResponseController skeleton
```csharp
// Source: derived from existing AyaSampleController pattern + research analysis
using System.Collections;
using System.Collections.Generic;
using System.Text;
using UnityEngine;
using UnityEngine.InputSystem;
using AIEmbodiment;

namespace AIEmbodiment.Samples
{
    public enum QueuedState
    {
        Connecting,
        Idle,
        Recording,
        Reviewing,
        Playing
    }

    public class QueuedResponseController : MonoBehaviour
    {
        [SerializeField] private PersonaSession _session;
        [SerializeField] private AudioPlayback _audioPlayback;
        [SerializeField] private QueuedResponseUI _ui;

        private QueuedState _state = QueuedState.Connecting;
        private readonly List<float[]> _audioBuffer = new List<float[]>();
        private readonly StringBuilder _aiTranscript = new StringBuilder();
        private bool _turnComplete;
        private bool _playbackInitialized;

        private void Start()
        {
            _session.OnStateChanged += HandleStateChanged;
            _session.OnInputTranscription += HandleInputTranscription;
            _session.OnOutputTranscription += HandleOutputTranscription;
            _session.OnSyncPacket += HandleSyncPacket;
            _session.OnTurnComplete += HandleTurnComplete;

            _session.Connect();
        }

        private void Update()
        {
            if (Keyboard.current == null) return;

            switch (_state)
            {
                case QueuedState.Idle:
                    if (Keyboard.current.spaceKey.wasPressedThisFrame)
                        EnterRecording();
                    break;

                case QueuedState.Recording:
                    if (Keyboard.current.spaceKey.wasReleasedThisFrame)
                        EnterReviewing();
                    break;

                case QueuedState.Reviewing:
                    if (Keyboard.current.enterKey.wasPressedThisFrame)
                        EnterPlaying();
                    else if (Keyboard.current.escapeKey.wasPressedThisFrame)
                        DiscardAndReturnToIdle();
                    break;

                case QueuedState.Playing:
                    // Check if playback is complete
                    if (_turnComplete && !_audioPlayback.IsPlaying)
                        EnterIdle();
                    break;
            }
        }

        private void EnterRecording()
        {
            _state = QueuedState.Recording;
            _audioBuffer.Clear();
            _aiTranscript.Clear();
            _turnComplete = false;
            _audioPlayback?.ClearBuffer();
            _session.StartListening();
            _ui.SetState(QueuedState.Recording);
        }

        private void EnterReviewing()
        {
            _state = QueuedState.Reviewing;
            _session.StopListening();
            _ui.SetState(QueuedState.Reviewing);
            // AI response starts streaming in background -- audio buffered via HandleSyncPacket
        }

        private void EnterPlaying()
        {
            _state = QueuedState.Playing;
            EnsurePlaybackInitialized();

            // Feed all buffered audio to AudioPlayback
            foreach (var chunk in _audioBuffer)
            {
                _audioPlayback.EnqueueAudio(chunk);
            }
            _audioBuffer.Clear();

            _ui.SetState(QueuedState.Playing);
            _ui.SetAITranscript(_aiTranscript.ToString());
        }

        private void EnterIdle()
        {
            _state = QueuedState.Idle;
            _ui.SetState(QueuedState.Idle);
        }

        private void DiscardAndReturnToIdle()
        {
            _audioBuffer.Clear();
            _aiTranscript.Clear();
            _turnComplete = false;
            _audioPlayback?.ClearBuffer();
            _state = QueuedState.Idle;
            _ui.SetState(QueuedState.Idle);
        }

        private void EnsurePlaybackInitialized()
        {
            if (!_playbackInitialized && _audioPlayback != null)
            {
                _audioPlayback.Initialize();
                _playbackInitialized = true;
            }
        }

        // --- Event Handlers ---

        private void HandleSyncPacket(SyncPacket packet)
        {
            if (packet.Type != SyncPacketType.TextAudio) return;

            if (_state == QueuedState.Reviewing)
            {
                // Buffer audio for deferred playback
                if (packet.Audio != null && packet.Audio.Length > 0)
                    _audioBuffer.Add(packet.Audio);
            }
            else if (_state == QueuedState.Playing)
            {
                // Feed directly to playback (AI still streaming)
                if (packet.Audio != null && packet.Audio.Length > 0)
                {
                    EnsurePlaybackInitialized();
                    _audioPlayback.EnqueueAudio(packet.Audio);
                }
            }
        }

        private void HandleOutputTranscription(string text)
        {
            if (_state == QueuedState.Reviewing || _state == QueuedState.Playing)
            {
                _aiTranscript.Append(text);
                if (_state == QueuedState.Playing)
                    _ui.SetAITranscript(_aiTranscript.ToString());
            }
        }

        private void HandleInputTranscription(string text)
        {
            if (_state == QueuedState.Recording || _state == QueuedState.Reviewing)
                _ui.SetUserTranscript(text);
        }

        private void HandleTurnComplete()
        {
            _turnComplete = true;
            if (_state == QueuedState.Reviewing)
                _ui.ShowReadyIndicator(); // "AI response ready!"
        }

        private void HandleStateChanged(SessionState state)
        {
            if (state == SessionState.Connected)
            {
                _state = QueuedState.Idle;
                _ui.SetState(QueuedState.Idle);
            }
        }
    }
}
```

### QueuedResponseUI skeleton
```csharp
// Source: derived from existing AyaChatUI pattern
using UnityEngine;
using UnityEngine.UIElements;

namespace AIEmbodiment.Samples
{
    public class QueuedResponseUI : MonoBehaviour
    {
        [SerializeField] private UIDocument _uiDocument;

        private Label _userTranscript;
        private Label _aiResponse;
        private Label _statusLabel;
        private Label _keyHints;
        private VisualElement _stateIndicator;

        private void OnEnable()
        {
            var root = _uiDocument.rootVisualElement;
            _userTranscript = root.Q<Label>("user-transcript");
            _aiResponse = root.Q<Label>("ai-response");
            _statusLabel = root.Q<Label>("status-label");
            _keyHints = root.Q<Label>("key-hints");
            _stateIndicator = root.Q("state-indicator");
        }

        public void SetState(QueuedState state)
        {
            _statusLabel.text = state switch
            {
                QueuedState.Connecting => "Connecting to AI...",
                QueuedState.Idle => "Ready. Hold SPACE to talk.",
                QueuedState.Recording => "Recording... Release SPACE when done.",
                QueuedState.Reviewing => "Review your message. ENTER to send, ESC to cancel.",
                QueuedState.Playing => "AI is responding...",
                _ => ""
            };

            _keyHints.text = state switch
            {
                QueuedState.Idle => "[SPACE] Record",
                QueuedState.Recording => "[SPACE] Release to stop",
                QueuedState.Reviewing => "[ENTER] Send  [ESC] Cancel",
                QueuedState.Playing => "Listening to response...",
                _ => ""
            };
        }

        public void SetUserTranscript(string text)
        {
            _userTranscript.text = text;
        }

        public void SetAITranscript(string text)
        {
            _aiResponse.text = text;
        }

        public void ShowReadyIndicator()
        {
            // Visual indicator that AI response is fully buffered
            _stateIndicator?.AddToClassList("response-ready");
        }
    }
}
```

### UXML layout skeleton
```xml
<ui:UXML xmlns:ui="UnityEngine.UIElements">
    <ui:VisualElement name="root-container" class="root">
        <ui:VisualElement name="header" class="header">
            <ui:VisualElement name="state-indicator" class="indicator" />
            <ui:Label name="status-label" text="Connecting..." class="status-label" />
        </ui:VisualElement>

        <ui:VisualElement name="transcript-area" class="transcript-area">
            <ui:Label text="You:" class="section-header" />
            <ui:Label name="user-transcript" text="" class="transcript user-text" />
        </ui:VisualElement>

        <ui:VisualElement name="response-area" class="response-area">
            <ui:Label text="AI:" class="section-header" />
            <ui:Label name="ai-response" text="" class="transcript ai-text" />
        </ui:VisualElement>

        <ui:VisualElement name="footer" class="footer">
            <ui:Label name="key-hints" text="" class="key-hints" />
        </ui:VisualElement>
    </ui:VisualElement>
</ui:UXML>
```

## State of the Art

| Old Approach (AyaLiveStream) | New Approach (QueuedResponse) | Why Different |
|-----|-----|-----|
| PersonaSession wired to AudioPlayback (auto-play) | PersonaSession `_audioPlayback` null (manual play) | Decouple audio arrival from playback |
| Live streaming UX (hear AI immediately) | Queued approval UX (hear AI after Enter) | Different UX paradigm |
| Single state: connected or not | Five-state machine: Connecting/Idle/Recording/Reviewing/Playing | Complex UX flow |
| OnSyncPacket for logging only | OnSyncPacket as primary audio data source | Controller manages audio routing |
| Chat log UI (scrolling history) | Two-panel transcript UI (current turn only) | Focus on single turn approval flow |

## Open Questions

1. **Should Escape during Reviewing actually cancel the AI's pending response?**
   - What we know: Gemini Live has no explicit "cancel response" message. The server will complete its turn regardless.
   - What's unclear: Whether discarding the response locally and starting fresh could confuse the conversation history on the server.
   - Recommendation: Discard locally (clear buffers). The orphaned response completes on the server and becomes part of conversation history. This is acceptable -- the next user message will naturally redirect the conversation. No special cancellation needed.

2. **Should the AI response transcript be hidden until Playing state or shown during Reviewing?**
   - What we know: The success criteria say "AI response transcript displays alongside audio playback" (criterion 4).
   - What's unclear: Whether showing transcript during Reviewing (before approval) is desired.
   - Recommendation: Show AI transcript only during Playing state to match the success criteria and to avoid "spoiling" the response before the user commits to hearing it. During Reviewing, show only a "AI response ready!" indicator when `_turnComplete` is true.

3. **AudioPlayback initialization timing**
   - What we know: Initialize() must be called before EnqueueAudio(). It creates a dummy clip and starts the AudioSource loop.
   - What's unclear: Whether calling Initialize() in Start() (eagerly) or on first approval (lazily) is better.
   - Recommendation: Call Initialize() eagerly in Start() after the AudioPlayback reference is set. This avoids any timing issues. The AudioSource plays a silent loop (zero resource cost) until real audio is enqueued.

4. **PersonaConfig reuse vs. new config**
   - What we know: AyaPersonaConfig already exists and is configured correctly.
   - What's unclear: Whether the queued sample should reuse Aya's persona or define its own.
   - Recommendation: Reuse AyaPersonaConfig. The sample is about the UX pattern, not the persona. Referencing the same config avoids duplication and shows that the same PersonaSession API serves different UX patterns.

## Sources

### Primary (HIGH confidence)
- Direct examination of `Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs` -- verified null-conditional `_audioPlayback` handling, event flow, SyncPacket routing
- Direct examination of `Packages/com.google.ai-embodiment/Runtime/AudioPlayback.cs` -- verified Initialize/EnqueueAudio/ClearBuffer/IsPlaying API, ring buffer lifecycle
- Direct examination of `Packages/com.google.ai-embodiment/Runtime/PacketAssembler.cs` -- verified audio still flows through AddAudio when _audioPlayback is null
- Direct examination of `Packages/com.google.ai-embodiment/Runtime/GeminiLiveClient.cs` -- verified event types, inputTranscription behavior
- Direct examination of `Packages/com.google.ai-embodiment/Runtime/AudioRingBuffer.cs` -- verified capacity (30s at 24kHz), overflow behavior
- Direct examination of `Assets/AyaLiveStream/AyaSampleController.cs` -- reference for controller pattern
- Direct examination of `Assets/AyaLiveStream/AyaChatUI.cs` -- reference for UI pattern
- Direct examination of `Assets/AyaLiveStream/UI/AyaPanel.uxml` and `.uss` -- reference for UI Toolkit patterns

### Secondary (MEDIUM confidence)
- Understanding of Unity UI Toolkit element naming/querying patterns from training data -- consistent with observed code
- Understanding of Unity InputSystem keyboard API from training data -- consistent with existing AyaSampleController usage

## Metadata

**Confidence breakdown:**
- Null-AudioPlayback decoupling pattern: HIGH -- directly verified in source code with line numbers
- State machine design: HIGH -- derived from success criteria with full event/input mapping
- Audio buffering strategy: HIGH -- verified from AudioPlayback and PacketAssembler APIs
- Edge case handling: HIGH -- each case traced through PersonaSession event flow
- UI layout: MEDIUM -- follows established patterns but specific element choices are discretionary
- Scene hierarchy: HIGH -- follows existing AyaLiveStream pattern with documented modifications

**Research date:** 2026-02-13
**Valid until:** indefinite -- this is codebase-internal research, not dependent on external libraries
