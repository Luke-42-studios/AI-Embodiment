---
phase: 02-audio-pipeline
plan: 03
type: execute
wave: 2
depends_on: [02-01, 02-02]
files_modified:
  - Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs
autonomous: true

must_haves:
  truths:
    - "PersonaSession.StartListening() begins mic capture and streams audio to Gemini Live"
    - "PersonaSession.StopListening() stops mic capture"
    - "AI voice audio from Gemini plays through AudioPlayback in real time as chunks arrive"
    - "Interruption clears the audio buffer so stale AI speech stops immediately"
    - "User input transcript (speech-to-text) fires via OnInputTranscription event"
    - "Audio components are optional -- if not assigned, session falls back to text-only mode"
    - "PersonaSession fires OnAISpeakingStarted/OnAISpeakingStopped and OnUserSpeakingStarted/OnUserSpeakingStopped events"
    - "Disconnect auto-stops AudioCapture and AudioPlayback"
  artifacts:
    - path: "Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs"
      provides: "Audio-integrated session with push-to-talk, audio routing, and speaking events"
      contains: "StartListening"
  key_links:
    - from: "PersonaSession.StartListening"
      to: "AudioCapture.StartCapture"
      via: "push-to-talk API delegates to capture component"
      pattern: "_audioCapture\\.StartCapture"
    - from: "AudioCapture.OnAudioCaptured"
      to: "LiveSession.SendAudioAsync"
      via: "PersonaSession subscribes to callback and forwards to SDK"
      pattern: "SendAudioAsync"
    - from: "PersonaSession.ProcessResponse"
      to: "AudioPlayback.EnqueueAudio"
      via: "routes AudioAsFloat from response to playback ring buffer"
      pattern: "_audioPlayback\\.EnqueueAudio"
    - from: "PersonaSession.ProcessResponse (Interrupted)"
      to: "AudioPlayback.ClearBuffer"
      via: "clears ring buffer on barge-in"
      pattern: "_audioPlayback\\.ClearBuffer"
    - from: "PersonaSession.Disconnect"
      to: "AudioCapture.StopCapture + AudioPlayback.Stop"
      via: "clean teardown on disconnect"
      pattern: "StopCapture|_audioPlayback\\.Stop"
---

<objective>
Integrate AudioCapture and AudioPlayback into PersonaSession to complete the end-to-end voice loop: user speaks into mic, audio streams to Gemini Live, AI voice response plays back through AudioSource.

Purpose: This plan wires together the two independent audio components (from Plans 02-01 and 02-02) through PersonaSession, adding push-to-talk API, audio routing in the receive loop, speaking state events, and clean teardown. After this plan, the full bidirectional audio pipeline works.

Output: Modified PersonaSession.cs with audio integration, completing all Phase 2 requirements (AUDIO-01 through AUDIO-04, VOICE-01, TRNS-01).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-audio-pipeline/02-RESEARCH.md
@.planning/phases/02-audio-pipeline/02-CONTEXT.md
@.planning/phases/02-audio-pipeline/02-01-SUMMARY.md
@.planning/phases/02-audio-pipeline/02-02-SUMMARY.md
@Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs
@Packages/com.google.ai-embodiment/Runtime/AudioCapture.cs
@Packages/com.google.ai-embodiment/Runtime/AudioPlayback.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: PersonaSession audio fields, push-to-talk API, and speaking events</name>
  <files>Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs</files>
  <action>
Modify existing PersonaSession.cs to add audio component references, push-to-talk API, and speaking state events. Read the current file first.

**Add SerializeField references (after existing `_config` field):**
```csharp
[SerializeField] private AudioCapture _audioCapture;   // optional
[SerializeField] private AudioPlayback _audioPlayback;  // optional
```

**Add new events (alongside existing events):**
```csharp
/// <summary>Fires when the AI starts producing audio output.</summary>
public event Action OnAISpeakingStarted;

/// <summary>Fires when the AI finishes producing audio output (after buffer drains).</summary>
public event Action OnAISpeakingStopped;

/// <summary>Fires when the user starts speaking (first audio chunk sent after StartListening).</summary>
public event Action OnUserSpeakingStarted;

/// <summary>Fires when the user stops speaking (StopListening called).</summary>
public event Action OnUserSpeakingStopped;
```

**Add private state tracking:**
```csharp
private bool _aiSpeaking;
private bool _userSpeaking;
private bool _isListening;
```

**Add push-to-talk API methods (CONTEXT.md: "Push-to-talk API lives on PersonaSession"):**

`StartListening()`:
1. If `_audioCapture == null`: `Debug.LogWarning("PersonaSession: No AudioCapture assigned -- cannot listen.")`, return.
2. If `State != SessionState.Connected`: warn and return.
3. If `_isListening`: return (already listening).
4. Set `_isListening = true`.
5. Subscribe to `_audioCapture.OnAudioCaptured += HandleAudioCaptured`.
6. Call `_audioCapture.StartCapture()`.

`StopListening()`:
1. If not `_isListening`: return.
2. Set `_isListening = false`.
3. Call `_audioCapture.StopCapture()`.
4. Unsubscribe `_audioCapture.OnAudioCaptured -= HandleAudioCaptured`.
5. If `_userSpeaking`: set `_userSpeaking = false`, fire `OnUserSpeakingStopped?.Invoke()`.

`HandleAudioCaptured(float[] chunk)` -- private method, subscribed to AudioCapture callback:
1. If `_liveSession == null || State != SessionState.Connected`: return.
2. If not `_userSpeaking`: set `_userSpeaking = true`, fire `OnUserSpeakingStarted?.Invoke()`.
3. Fire-and-forget send: `_ = _liveSession.SendAudioAsync(chunk, _sessionCts.Token);`
   - This runs on background thread via the SDK (Research: SDK handles float->PCM->base64->JSON->WebSocket).

**Initialize AudioPlayback in Connect():**
After the line `SetState(SessionState.Connected);`, add:
```csharp
if (_audioPlayback != null)
{
    _audioPlayback.Initialize();
}
```

**Modify Disconnect():**
After `_sessionCts?.Cancel();` and before the CloseAsync block, add:
```csharp
// Stop audio components (CONTEXT.md: "PersonaSession auto-stops on disconnect")
if (_isListening)
{
    StopListening();
}
if (_audioPlayback != null)
{
    _audioPlayback.Stop();
}
_aiSpeaking = false;
```

**Modify OnDestroy():**
Before `_sessionCts?.Cancel();`, add:
```csharp
if (_isListening && _audioCapture != null)
{
    _audioCapture.StopCapture();
    _audioCapture.OnAudioCaptured -= HandleAudioCaptured;
}
_audioPlayback?.Stop();
```
  </action>
  <verify>
Open PersonaSession.cs and verify:
1. _audioCapture and _audioPlayback are [SerializeField] private
2. StartListening/StopListening exist with guard checks
3. HandleAudioCaptured calls SendAudioAsync with the chunk
4. Four new speaking events declared
5. AudioPlayback.Initialize() called after Connected
6. Disconnect stops both audio components
7. OnDestroy stops capture and playback
  </verify>
  <done>PersonaSession has push-to-talk API (StartListening/StopListening), audio component SerializeField references, speaking state events, AudioPlayback initialization on connect, and clean audio teardown on disconnect and OnDestroy</done>
</task>

<task type="auto">
  <name>Task 2: PersonaSession ProcessResponse audio routing -- Gemini audio to playback and transcript events</name>
  <files>Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs</files>
  <action>
Modify the existing `ProcessResponse` method in PersonaSession.cs to route audio data to AudioPlayback and handle speaking state transitions.

**Replace the current ProcessResponse method** with an enhanced version that adds audio routing. Preserve ALL existing behavior (text, turn complete, interrupted, transcriptions, tool call placeholder).

**Audio routing additions (add BEFORE existing text handling, inside the `if (response.Message is LiveSessionContent content)` block):**

```csharp
// Route audio to playback component (VOICE-01: Gemini native audio path)
var audioChunks = response.AudioAsFloat;
if (audioChunks != null && audioChunks.Count > 0 && _audioPlayback != null)
{
    foreach (var chunk in audioChunks)
    {
        _audioPlayback.EnqueueAudio(chunk);
    }
    // Track AI speaking state
    if (!_aiSpeaking)
    {
        _aiSpeaking = true;
        MainThreadDispatcher.Enqueue(() => OnAISpeakingStarted?.Invoke());
    }
}
```

**Enhance the existing `content.Interrupted` block:**
Currently it just fires OnInterrupted. Enhance to also clear audio buffer:
```csharp
if (content.Interrupted)
{
    // Clear buffered audio on barge-in (Research Pitfall 9)
    if (_audioPlayback != null)
    {
        _audioPlayback.ClearBuffer();
    }
    if (_aiSpeaking)
    {
        _aiSpeaking = false;
        MainThreadDispatcher.Enqueue(() => OnAISpeakingStopped?.Invoke());
    }
    MainThreadDispatcher.Enqueue(() => OnInterrupted?.Invoke());
}
```

**Enhance the existing `content.TurnComplete` block:**
Currently it just fires OnTurnComplete. Enhance to also signal AI speaking stopped:
```csharp
if (content.TurnComplete)
{
    if (_aiSpeaking)
    {
        _aiSpeaking = false;
        MainThreadDispatcher.Enqueue(() => OnAISpeakingStopped?.Invoke());
    }
    MainThreadDispatcher.Enqueue(() => OnTurnComplete?.Invoke());
}
```

**Existing behavior that MUST be preserved unchanged:**
- Text routing: `OnTextReceived?.Invoke(text)` when text is non-empty
- `OnInputTranscription?.Invoke(transcript)` when InputTranscription has value (TRNS-01 -- already implemented in Phase 1!)
- `OnOutputTranscription?.Invoke(transcript)` when OutputTranscription has value
- `LiveSessionToolCall` placeholder log for Phase 4

**Important from RESEARCH.md:**
- `response.AudioAsFloat` returns `IReadOnlyList<float[]>` -- SDK already decoded from base64 PCM bytes (Don't Hand-Roll)
- Audio arrives at 24kHz mono -- AudioPlayback handles resampling
- On Interrupted: clear ring buffer to stop stale audio (Pitfall 9)
- ProcessResponse runs on BACKGROUND THREAD -- all event invocations via MainThreadDispatcher
- Capture audio chunk references into locals before lambda (established pattern from Phase 1)

**Do NOT:**
- Touch OnAudioFilterRead or ring buffer code -- that's AudioPlayback's responsibility
- Add client-side voice activity detection -- server-side barge-in only (CONTEXT.md)
- Change the ReceiveLoopAsync outer while loop structure
- Remove or modify existing event firings (text, turn complete, input/output transcription)
  </action>
  <verify>
Open PersonaSession.cs and verify:
1. ProcessResponse checks response.AudioAsFloat and routes to _audioPlayback.EnqueueAudio
2. AI speaking state tracked: OnAISpeakingStarted on first audio chunk, OnAISpeakingStopped on TurnComplete/Interrupted
3. Interrupted block calls _audioPlayback.ClearBuffer()
4. All existing behavior preserved: OnTextReceived, OnTurnComplete, OnInterrupted, OnInputTranscription, OnOutputTranscription
5. All dispatches go through MainThreadDispatcher
6. No direct Unity API calls in ProcessResponse (runs on background thread)
  </verify>
  <done>ProcessResponse routes AudioAsFloat to AudioPlayback, tracks AI speaking state with start/stop events, clears audio buffer on interruption, and preserves all existing text/transcription/turn-complete behavior. End-to-end voice loop is complete: mic -> Gemini -> playback.</done>
</task>

</tasks>

<verification>
- PersonaSession.StartListening() calls AudioCapture.StartCapture() and wires OnAudioCaptured to SendAudioAsync
- PersonaSession.StopListening() stops capture and fires user speaking stopped
- ProcessResponse routes AudioAsFloat chunks to AudioPlayback.EnqueueAudio
- Interrupted clears ring buffer via AudioPlayback.ClearBuffer()
- TurnComplete fires OnAISpeakingStopped
- Disconnect and OnDestroy stop both audio components
- Audio components are optional -- null checks everywhere, text-only mode preserved
- All events dispatched via MainThreadDispatcher (background thread safety)
- OnInputTranscription fires for user speech-to-text (TRNS-01)
- Full requirement coverage: AUDIO-01, AUDIO-02, AUDIO-03, AUDIO-04, VOICE-01, TRNS-01
</verification>

<success_criteria>
- StartListening/StopListening work as push-to-talk API on PersonaSession
- Audio captured from mic streams to Gemini via SendAudioAsync
- Audio from Gemini response plays through AudioPlayback
- Speaking state events fire correctly (AI and user)
- Interruption clears audio buffer
- Audio components are optional (null-safe)
- Disconnect cleans up both audio components
- All Phase 2 requirements (AUDIO-01 through AUDIO-04, VOICE-01, TRNS-01) are addressed
</success_criteria>

<output>
After completion, create `.planning/phases/02-audio-pipeline/02-03-SUMMARY.md`
</output>
