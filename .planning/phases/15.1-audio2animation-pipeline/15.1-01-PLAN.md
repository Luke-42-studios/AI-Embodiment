---
phase: 15.1-audio2animation-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Packages/com.google.ai-embodiment/Runtime/IAnimationModel.cs
  - Packages/com.google.ai-embodiment/Runtime/Audio2Animation.cs
  - Packages/com.google.ai-embodiment/Runtime/FakeAnimationModel.cs
  - Packages/com.google.ai-embodiment/Runtime/BlendshapeAnimationConverter.cs
autonomous: true

must_haves:
  truths:
    - "Audio2Animation accepts a SyncPacket and forwards its audio to the IAnimationModel"
    - "FakeAnimationModel streams frames paced by audio duration budget, not dumped all at once"
    - "IAnimationModel interface allows swapping FakeAnimationModel for a real ML model without changing Audio2Animation"
    - "FakeAnimationModel loops pre-recorded data when frames run out (wraps index)"
    - "Cancel() immediately stops frame emission and clears budget"
  artifacts:
    - path: "Packages/com.google.ai-embodiment/Runtime/IAnimationModel.cs"
      provides: "Pluggable animation model interface"
      exports: ["IAnimationModel"]
    - path: "Packages/com.google.ai-embodiment/Runtime/Audio2Animation.cs"
      provides: "Orchestrator wiring audio to model to frame events"
      exports: ["Audio2Animation"]
    - path: "Packages/com.google.ai-embodiment/Runtime/FakeAnimationModel.cs"
      provides: "Pre-recorded JSON frame streaming synchronized to audio timing"
      exports: ["FakeAnimationModel"]
    - path: "Packages/com.google.ai-embodiment/Runtime/BlendshapeAnimationConverter.cs"
      provides: "Public NormalizeBlendshapeName and FindFuzzyBlendshapeMatch"
      contains: "public static string NormalizeBlendshapeName"
  key_links:
    - from: "Audio2Animation.cs"
      to: "IAnimationModel"
      via: "constructor injection"
      pattern: "IAnimationModel _model"
    - from: "Audio2Animation.cs"
      to: "SyncPacket"
      via: "ProcessPacket method"
      pattern: "ProcessPacket.*SyncPacket"
    - from: "FakeAnimationModel.cs"
      to: "BlendshapeAnimationData"
      via: "constructor injection of pre-recorded data"
      pattern: "BlendshapeAnimationData"
    - from: "FakeAnimationModel.cs"
      to: "Audio2Animation.OnFrameReady"
      via: "callback Action<BlendshapeFrame>"
      pattern: "Action<BlendshapeFrame>"
---

<objective>
Create the Audio2Animation pipeline runtime classes: IAnimationModel interface for pluggability, Audio2Animation orchestrator that processes SyncPackets and emits BlendshapeFrame events, and FakeAnimationModel that streams pre-recorded JSON animation data synchronized to audio timing via a time-accumulator pattern. Also make BlendshapeAnimationConverter's fuzzy matching methods public for reuse by downstream consumers.

Purpose: Establishes the core pipeline that converts audio chunks into streaming animation frames -- the foundation that the sample app (Plan 02) will consume.
Output: 3 new C# files in package runtime + 1 modified file with public fuzzy matching.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15.1-audio2animation-pipeline/15.1-RESEARCH.md

Key source files to read before implementation:
@Packages/com.google.ai-embodiment/Runtime/SyncPacket.cs
@Packages/com.google.ai-embodiment/Runtime/BlendshapeAnimationData.cs
@Packages/com.google.ai-embodiment/Runtime/BlendshapeAnimationConverter.cs
@Packages/com.google.ai-embodiment/Runtime/PacketAssembler.cs
@Packages/com.google.ai-embodiment/Runtime/ISyncDriver.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: IAnimationModel interface and Audio2Animation orchestrator</name>
  <files>
    Packages/com.google.ai-embodiment/Runtime/IAnimationModel.cs
    Packages/com.google.ai-embodiment/Runtime/Audio2Animation.cs
  </files>
  <action>
    Create two plain C# classes (NOT MonoBehaviour) in the AIEmbodiment namespace, following the PacketAssembler pattern (plain C# class, no Unity lifecycle).

    **IAnimationModel.cs:**
    - Interface with two methods:
      - `void ProcessAudio(float[] audioSamples, int sampleRate)` -- accepts PCM audio chunk and produces frames over time
      - `void Cancel()` -- stops any in-progress frame streaming (barge-in support)
    - Add XML doc comments explaining that ProcessAudio may produce frames asynchronously over time (not all at once), and that implementations must use a Tick() pattern driven by external caller.
    - NOTE: Tick() is NOT part of the interface because it is an implementation detail of time-based models. A real ML model might emit frames via callback directly from ProcessAudio. The interface only defines the audio input contract.

    **Audio2Animation.cs:**
    - Plain C# class (not MonoBehaviour), constructor takes `IAnimationModel model`
    - `public event Action<BlendshapeFrame> OnFrameReady` -- fires for each animation frame produced
    - `public void ProcessPacket(SyncPacket packet)` -- extracts audio from TextAudio packets, forwards to model. Guard: skip if `packet.Type != SyncPacketType.TextAudio` or `packet.Audio == null || packet.Audio.Length == 0`. Pass `24000` as sampleRate (Gemini Live standard).
    - `public void Cancel()` -- delegates to `_model.Cancel()`
    - Audio2Animation must wire the model's frame output to OnFrameReady. Since IAnimationModel does not have an event (it's an interface -- implementations vary), Audio2Animation must provide a way for the model to emit frames. Solution: pass an `Action<BlendshapeFrame> onFrameReady` callback to the model. Add a `SetFrameCallback(Action<BlendshapeFrame> callback)` method to IAnimationModel, OR have Audio2Animation set it via a property/method on the model. PREFERRED approach: Add `void SetFrameCallback(Action<BlendshapeFrame> callback)` to IAnimationModel. Audio2Animation calls `_model.SetFrameCallback(frame => OnFrameReady?.Invoke(frame))` in its constructor.
    - Do NOT implement ISyncDriver. Do NOT register with PacketAssembler. This is a downstream consumer.
  </action>
  <verify>
    Files compile with no errors: check that `IAnimationModel.cs` and `Audio2Animation.cs` exist in the Runtime directory and use the `AIEmbodiment` namespace. Verify `Audio2Animation` references `SyncPacket`, `SyncPacketType`, and `BlendshapeFrame` correctly.
  </verify>
  <done>
    IAnimationModel interface defines ProcessAudio, Cancel, and SetFrameCallback. Audio2Animation orchestrates SyncPacket -> IAnimationModel -> OnFrameReady event chain. Both files exist in package runtime with proper namespace.
  </done>
</task>

<task type="auto">
  <name>Task 2: FakeAnimationModel and public fuzzy matching</name>
  <files>
    Packages/com.google.ai-embodiment/Runtime/FakeAnimationModel.cs
    Packages/com.google.ai-embodiment/Runtime/BlendshapeAnimationConverter.cs
  </files>
  <action>
    **FakeAnimationModel.cs:**
    Create a plain C# class implementing IAnimationModel in the AIEmbodiment namespace. This is the "fake model" that streams pre-recorded BlendshapeFrame data synchronized to audio timing.

    Constructor: `FakeAnimationModel(BlendshapeAnimationData data)`
    - Store `_data` (the pre-recorded animation data)
    - Calculate `_frameDuration` from the data: `_frameDuration = data.frames[0].timeSeconds` (this is the per-frame duration -- e.g., 0.0333s for 30fps, 0.04s for 25fps). If timeSeconds is 0 or data is empty, default to `1f / 30f`.
    - Initialize `_currentFrameIndex = 0`, `_animationBudgetSeconds = 0f`, `_timeAccumulator = 0f`
    - `_onFrameReady` callback (Action<BlendshapeFrame>) -- set via SetFrameCallback

    Implement `SetFrameCallback(Action<BlendshapeFrame> callback)`:
    - Store callback as `_onFrameReady`

    Implement `ProcessAudio(float[] audioSamples, int sampleRate)`:
    - Calculate audio duration: `float audioDuration = (float)audioSamples.Length / sampleRate`
    - Add to budget: `_animationBudgetSeconds += audioDuration`
    - That's it -- frames are emitted in Tick(), not here

    Implement `Cancel()`:
    - Set `_animationBudgetSeconds = 0f`
    - Set `_timeAccumulator = 0f`
    - (Do NOT reset _currentFrameIndex -- let it continue from where it left off on next audio)

    Add public method `Tick(float deltaTime)`:
    - If `_animationBudgetSeconds <= 0f || _data.frames.Count == 0`, return
    - `_timeAccumulator += deltaTime`
    - While loop: `while (_timeAccumulator >= _frameDuration && _animationBudgetSeconds > 0f)`
      - Emit frame: `_onFrameReady?.Invoke(_data.frames[_currentFrameIndex % _data.frames.Count])` (modulo wraps when data runs out -- Pitfall 5)
      - `_currentFrameIndex++`
      - `_timeAccumulator -= _frameDuration`
      - `_animationBudgetSeconds -= _frameDuration`
    - Clamp budget to 0: `if (_animationBudgetSeconds < 0f) _animationBudgetSeconds = 0f`

    IMPORTANT: Tick() is called by the external MonoBehaviour (FaceAnimationPlayer in Plan 02) every frame with Time.deltaTime. This class cannot own coroutines -- it's plain C#.

    **BlendshapeAnimationConverter.cs modification:**
    Change two methods from `private static` to `public static`:
    - `NormalizeBlendshapeName(string name)` -- change from `private static string` to `public static string`
    - `FindFuzzyBlendshapeMatch(string sourceName, HashSet<string> availableNames)` -- change from `private static string` to `public static string`

    These are needed by FaceAnimationPlayer (Plan 02) for building the blendshape index map at initialization. No other changes to BlendshapeAnimationConverter.
  </action>
  <verify>
    Files compile: FakeAnimationModel.cs exists and implements IAnimationModel. BlendshapeAnimationConverter.NormalizeBlendshapeName and FindFuzzyBlendshapeMatch are now public static. Verify FakeAnimationModel references BlendshapeAnimationData and BlendshapeFrame from BlendshapeAnimationData.cs correctly.
  </verify>
  <done>
    FakeAnimationModel streams pre-recorded frames using audio-duration budget and time-accumulator pattern. Frame index wraps with modulo for looping. Cancel clears budget. BlendshapeAnimationConverter exposes NormalizeBlendshapeName and FindFuzzyBlendshapeMatch as public for downstream reuse.
  </done>
</task>

</tasks>

<verification>
1. All 3 new files exist in `Packages/com.google.ai-embodiment/Runtime/`:
   - IAnimationModel.cs (interface with ProcessAudio, Cancel, SetFrameCallback)
   - Audio2Animation.cs (orchestrator with ProcessPacket, Cancel, OnFrameReady event)
   - FakeAnimationModel.cs (implements IAnimationModel with Tick-driven frame streaming)
2. BlendshapeAnimationConverter.NormalizeBlendshapeName is now `public static`
3. BlendshapeAnimationConverter.FindFuzzyBlendshapeMatch is now `public static`
4. All files use `namespace AIEmbodiment`
5. Audio2Animation does NOT implement ISyncDriver
6. FakeAnimationModel does NOT inherit MonoBehaviour
7. FakeAnimationModel.Tick() uses modulo for frame index wrapping
</verification>

<success_criteria>
- IAnimationModel defines the pluggable contract (ProcessAudio, Cancel, SetFrameCallback)
- Audio2Animation wires SyncPacket audio to IAnimationModel and exposes OnFrameReady event
- FakeAnimationModel accumulates audio-duration budget and drains it via Tick() at native FPS
- Frame index wraps when pre-recorded data runs out
- Cancel() immediately stops frame emission
- BlendshapeAnimationConverter fuzzy matching is public for Plan 02 to use
</success_criteria>

<output>
After completion, create `.planning/phases/15.1-audio2animation-pipeline/15.1-01-SUMMARY.md`
</output>
