# Phase 15.1: Audio2Animation Pipeline - Research

**Researched:** 2026-02-17
**Domain:** Unity real-time blendshape animation, ISyncDriver integration, streaming frame playback
**Confidence:** HIGH

## Summary

This phase adds an Audio2Animation pipeline to the package runtime that accepts audio chunks and emits streaming blendshape animation frames. For now, the "model" is faked -- it streams pre-recorded JSON animation data synchronized to audio timing. The sample application subscribes to these frames and applies them directly to SkinnedMeshRenderer(s) in real-time using `SetBlendShapeWeight`, bypassing the existing batch `AnimationClip` approach in BlendshapeAnimationConverter.

The existing codebase has strong infrastructure to build on: the ISyncDriver interface was explicitly designed for this use case (comments mention "Future AI Face Animation holds packets until blend shapes are computed"), BlendshapeAnimationData/BlendshapeFrame are already the correct data model, and the SampleData directory contains 4 pre-recorded animation JSON files at 25-30fps with 34-63 ARKit blendshapes. The critical architectural decision is that **Audio2Animation should NOT be a second ISyncDriver** because PacketAssembler only supports one driver (single `_syncDriver` field). Instead, Audio2Animation sits downstream -- it receives SyncPackets (which contain audio) and produces animation frames independently.

**Primary recommendation:** Build Audio2Animation as a plain C# class in the package runtime with an abstract/interface `IAnimationModel` for pluggability. The fake implementation streams pre-recorded BlendshapeFrame objects at the JSON's native FPS, time-gated to not outrun audio duration. The sample app applies frames via direct `SkinnedMeshRenderer.SetBlendShapeWeight()` calls in a MonoBehaviour that accumulates frames in a queue and drains them in Update().

## Standard Stack

### Core (Existing -- No New Libraries)

| Component | Location | Purpose | Status |
|-----------|----------|---------|--------|
| BlendshapeAnimationData | Runtime/BlendshapeAnimationData.cs | Frame data model (frames, blendshapes dict, timeSeconds) | EXISTS -- reuse as-is |
| BlendshapeFrame | Runtime/BlendshapeAnimationData.cs | Single frame: timeSeconds + Dictionary<string, float> | EXISTS -- reuse as-is |
| BlendshapeAnimationConverter | Runtime/BlendshapeAnimationConverter.cs | ParseJson() for loading JSON, fuzzy name matching | EXISTS -- reuse ParseJson and fuzzy matching |
| ISyncDriver | Runtime/ISyncDriver.cs | Sync timing interface for PacketAssembler | EXISTS -- do NOT register A2A as driver |
| SyncPacket | Runtime/SyncPacket.cs | Carries audio (float[] at 24kHz mono) per sentence | EXISTS -- A2A consumes these |
| PersonaSession | Runtime/PersonaSession.cs | OnSyncPacket event for receiving packets | EXISTS -- sample subscribes here |

### New (To Be Created)

| Component | Location | Purpose |
|-----------|----------|---------|
| IAnimationModel | Runtime/IAnimationModel.cs | Interface: accept audio, emit BlendshapeFrame stream |
| Audio2Animation | Runtime/Audio2Animation.cs | Orchestrator: receives audio, delegates to IAnimationModel, emits frames |
| FakeAnimationModel | Runtime/FakeAnimationModel.cs | Streams pre-recorded JSON frames synchronized to audio timing |
| FaceAnimationPlayer | Samples~/AyaLiveStream/ (or Assets/) | MonoBehaviour applying frames to SkinnedMeshRenderer in real-time |

### No External Dependencies Required

This phase uses only existing Unity APIs and Newtonsoft.Json (already a dependency). No new packages needed.

## Architecture Patterns

### Critical Finding: PacketAssembler Supports Only ONE Sync Driver

**Evidence:** `PacketAssembler._syncDriver` is a single `ISyncDriver` field (line 24), and `RegisterSyncDriver` overwrites it (line 59-67). The system was designed for "the highest-latency driver wins" but implements this as a single-driver replacement, not a chain.

**Implication:** Audio2Animation CANNOT be registered as a second ISyncDriver alongside ChirpTTS. The ISyncDriver concept is for gating packet release, not for downstream processing.

**Decision:** Audio2Animation operates DOWNSTREAM of the sync pipeline. It subscribes to `PersonaSession.OnSyncPacket` (or receives SyncPackets via direct wiring) and processes the audio data contained in each packet.

### Recommended Architecture

```
Gemini Audio/Text
       |
       v
  PacketAssembler  (buffers into SyncPackets at sentence boundaries)
       |
       v
  [Optional ISyncDriver]  (e.g., ChirpTTS gates timing)
       |
       v
  PersonaSession.OnSyncPacket  (fires to all subscribers)
       |
       +---> AyaSampleController (existing: subtitles, function calls)
       |
       +---> Audio2Animation (NEW: receives SyncPacket.Audio)
                   |
                   v
            IAnimationModel.ProcessAudio(audioChunk, sampleRate)
                   |
                   v
            OnFrameReady event  (fires per BlendshapeFrame)
                   |
                   v
            FaceAnimationPlayer (MonoBehaviour in sample app)
                   |
                   v
            SkinnedMeshRenderer.SetBlendShapeWeight() per frame in Update()
```

### Recommended Project Structure

```
Packages/com.google.ai-embodiment/Runtime/
  IAnimationModel.cs          # Interface for pluggable model backends
  Audio2Animation.cs           # Orchestrator: audio in, frames out
  FakeAnimationModel.cs        # Pre-recorded JSON streaming implementation
  SampleData/
    animDemo1.json             # (existing) 269 frames, 30fps, 8.97s
    animDemo2.json             # (existing) 1786 frames, 25fps, 71.44s
    animDemo3.json             # (existing) 3106 frames, 25fps, 124.24s
    animDemo4.json             # (existing) 3106 frames, 25fps, 124.24s

Assets/AyaLiveStream/          # (or Samples~ equivalent)
  FaceAnimationPlayer.cs       # MonoBehaviour: queue frames, apply in Update()
```

### Pattern 1: IAnimationModel Interface (Pluggability)

**What:** Abstract the animation model so the fake can be swapped for a real ML model later.
**When to use:** Always -- this is a core requirement (Success Criterion 4).

```csharp
// Package Runtime: IAnimationModel.cs
namespace AIEmbodiment
{
    /// <summary>
    /// Pluggable backend for converting audio to facial animation frames.
    /// Implementations range from fake (pre-recorded playback) to real ML inference.
    /// </summary>
    public interface IAnimationModel
    {
        /// <summary>
        /// Process an audio chunk and produce animation frames.
        /// Frames are emitted via the OnFrameReady callback registered on Audio2Animation.
        ///
        /// IMPORTANT: This may produce frames asynchronously over time (not all at once).
        /// The fake model streams frames time-gated to audio duration.
        /// A real model would emit frames as inference completes.
        /// </summary>
        /// <param name="audioSamples">PCM audio samples (float[-1..1]).</param>
        /// <param name="sampleRate">Sample rate in Hz (typically 24000).</param>
        void ProcessAudio(float[] audioSamples, int sampleRate);

        /// <summary>
        /// Stop any in-progress frame streaming (e.g., on interruption/barge-in).
        /// </summary>
        void Cancel();
    }
}
```

### Pattern 2: Audio2Animation Orchestrator

**What:** Plain C# class (not MonoBehaviour) that wires IAnimationModel to the SyncPacket stream.
**When to use:** This is the core pipeline class.

```csharp
// Package Runtime: Audio2Animation.cs
namespace AIEmbodiment
{
    public class Audio2Animation
    {
        private IAnimationModel _model;

        /// <summary>Fires for each animation frame produced by the model.</summary>
        public event Action<BlendshapeFrame> OnFrameReady;

        public Audio2Animation(IAnimationModel model)
        {
            _model = model;
        }

        /// <summary>
        /// Feed a SyncPacket's audio into the animation model.
        /// Call this from OnSyncPacket handler.
        /// </summary>
        public void ProcessPacket(SyncPacket packet)
        {
            if (packet.Type != SyncPacketType.TextAudio) return;
            if (packet.Audio == null || packet.Audio.Length == 0) return;

            _model.ProcessAudio(packet.Audio, 24000);
        }

        /// <summary>Cancel in-progress animation (on barge-in).</summary>
        public void Cancel()
        {
            _model.Cancel();
        }
    }
}
```

### Pattern 3: FakeAnimationModel Frame Streaming

**What:** Streams pre-recorded JSON frames synchronized to audio duration.
**When to use:** This is the "fake model" implementation for Phase 15.1.

The critical synchronization question: How to pace frame emission to match audio duration.

**Approach:** When ProcessAudio receives an audio chunk, calculate its duration (`audioSamples.Length / sampleRate` seconds). Then stream that many seconds worth of pre-recorded frames at the JSON's native FPS. Use a time-accumulator pattern in an Update()-driven tick method (since FakeAnimationModel cannot own a coroutine -- it's not a MonoBehaviour).

```csharp
// Synchronization strategy for FakeAnimationModel:
//
// Audio chunk arrives: 24000 samples at 24kHz = 1.0 second
// Pre-recorded data: 30fps = need to emit 30 frames over 1.0 second
// Frame interval: 1/30 = 0.0333s
//
// Accumulate "animation budget" in seconds from audio chunks.
// In Tick(deltaTime), drain the budget by emitting frames at native FPS.
// This naturally paces frames to match audio arrival rate.
```

### Pattern 4: Real-Time BlendshapeFrame Application (NOT AnimationClip)

**What:** Apply individual frames directly via `SetBlendShapeWeight()` instead of creating AnimationClips.
**When to use:** Always for real-time streaming. AnimationClips are for batch/offline conversion.

```csharp
// Sample App: FaceAnimationPlayer.cs (MonoBehaviour)
//
// Initialization:
//   1. Find SkinnedMeshRenderer(s) on the face rig
//   2. Build a Dictionary<string, int> mapping blendshape names to indices
//      using Mesh.GetBlendShapeIndex(name) -- CACHE these at init, not per-frame
//   3. Use BlendshapeAnimationConverter.NormalizeBlendshapeName for fuzzy matching
//
// Per-frame in Update():
//   1. Dequeue next BlendshapeFrame from the frame queue
//   2. For each blendshape in the frame:
//      a. Look up cached index from name mapping
//      b. Call renderer.SetBlendShapeWeight(index, value * 100f)
//         (JSON values are 0-1, Unity expects 0-100)
//
// IMPORTANT: Weight conversion 0-1 -> 0-100 is already handled by
// BlendshapeAnimationConverter but must also be done here since we
// bypass that class for real-time application.
```

### Anti-Patterns to Avoid

- **Anti-pattern: Creating AnimationClips for real-time streaming.** The existing BlendshapeAnimationConverter creates AnimationClips (batch). For per-frame streaming, this would mean creating a new AnimationClip every 33ms -- massive GC pressure and totally wrong pattern. Use `SetBlendShapeWeight()` directly.

- **Anti-pattern: Registering Audio2Animation as a second ISyncDriver.** PacketAssembler only supports one driver. Registering would overwrite ChirpTTS. Audio2Animation is a downstream consumer, not a sync gate.

- **Anti-pattern: Using Coroutines in the package runtime class.** Audio2Animation and FakeAnimationModel are plain C# classes (not MonoBehaviour), following the PacketAssembler pattern. Frame timing must use a `Tick(float deltaTime)` method called by the MonoBehaviour consumer, not internal coroutines.

- **Anti-pattern: Dumping all frames at once.** Success Criterion 2 explicitly requires "synchronized to audio timing, not dumped all at once." The fake model must pace frame emission.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| JSON parsing | Custom JSON parser | `BlendshapeAnimationConverter.ParseJson()` | Already handles Newtonsoft.Json deserialization to BlendshapeAnimationData |
| Blendshape name fuzzy matching | String comparison logic | Reuse `NormalizeBlendshapeName` logic from BlendshapeAnimationConverter | Handles case, underscores, dots, namespace prefixes |
| Weight 0-1 to 0-100 conversion | Ad-hoc multiplication | Centralize as constant/helper | JSON uses 0-1, Unity uses 0-100 -- easy to forget |
| Thread-safe queue | Custom ring buffer | `ConcurrentQueue<BlendshapeFrame>` or simple `Queue<BlendshapeFrame>` | Everything runs on main thread (no audio thread involved for animation), so a plain Queue suffices |

**Key insight:** The existing BlendshapeAnimationConverter has excellent infrastructure (ParseJson, fuzzy matching, weight conversion) but its output format (AnimationClip) is wrong for real-time streaming. Reuse its parsing and matching utilities, but apply frames directly via SetBlendShapeWeight.

## Common Pitfalls

### Pitfall 1: PacketAssembler Single Driver Overwrite
**What goes wrong:** Registering Audio2Animation as ISyncDriver overwrites the existing ChirpTTS driver, breaking audio synchronization.
**Why it happens:** PacketAssembler._syncDriver is a single field, not a list. RegisterSyncDriver replaces, not chains.
**How to avoid:** Audio2Animation subscribes to OnSyncPacket downstream. It does NOT implement ISyncDriver.
**Warning signs:** ChirpTTS audio stops being synchronized; packets release immediately.

### Pitfall 2: Frame Emission Rate Mismatch with Audio Duration
**What goes wrong:** Pre-recorded animation plays too fast or too slow relative to actual speech audio.
**Why it happens:** The pre-recorded JSON files have fixed frame counts and FPS. Audio chunks from Gemini/TTS have variable duration.
**How to avoid:** Use audio duration as the pacing budget. Calculate `audioDurationSeconds = samples.Length / sampleRate`. Emit `audioDurationSeconds * animFPS` frames over that duration. If pre-recorded data runs out, loop or hold last frame.
**Warning signs:** Mouth movements finish before speech, or speech finishes while mouth still moves.

### Pitfall 3: Forgetting Weight Scale (0-1 vs 0-100)
**What goes wrong:** Blendshape weights appear nearly invisible (values 0-1 treated as 0-100 scale).
**Why it happens:** JSON animation data uses 0-1 range, Unity SetBlendShapeWeight expects 0-100.
**How to avoid:** Multiply by 100f when applying: `renderer.SetBlendShapeWeight(index, frame.blendshapes[name] * 100f)`.
**Warning signs:** Face barely moves despite receiving frames.

### Pitfall 4: GetBlendShapeIndex Called Every Frame
**What goes wrong:** Performance degrades with 63 string lookups per frame at 30fps.
**Why it happens:** Calling `Mesh.GetBlendShapeIndex(name)` is a string-based lookup.
**How to avoid:** Cache the Dictionary<string, int> mapping at initialization. Build it once from the SkinnedMeshRenderer's mesh blendshape list.
**Warning signs:** Profiler shows string operations dominating Update.

### Pitfall 5: FakeAnimationModel Running Out of Pre-recorded Data
**What goes wrong:** Animation stops mid-sentence because the pre-recorded JSON has fewer frames than the audio duration requires.
**Why it happens:** animDemo1.json is only 8.97s, but a sentence could be longer. animDemo2-4 are 71-124s.
**How to avoid:** Loop the pre-recorded data. When frame index exceeds data length, wrap to beginning. Or use the longest available file (animDemo3: 124s).
**Warning signs:** Face freezes mid-sentence, then resumes on next audio chunk.

### Pitfall 6: Not Cancelling Animation on Barge-In
**What goes wrong:** Old animation frames continue streaming after user interrupts the AI.
**Why it happens:** FakeAnimationModel is still emitting frames from its internal timer/budget.
**How to avoid:** Audio2Animation.Cancel() must clear the frame budget and any queued frames. Wire to PersonaSession.OnInterrupted.
**Warning signs:** Face continues animating after AI audio stops on interruption.

### Pitfall 7: NormalizeBlendshapeName is Private
**What goes wrong:** Cannot reuse fuzzy matching from BlendshapeAnimationConverter.
**Why it happens:** `NormalizeBlendshapeName` and `FindFuzzyBlendshapeMatch` are private static methods.
**How to avoid:** Either make them internal/public (package runtime change), or duplicate the normalization logic in FaceAnimationPlayer. Prefer making them public since we own the package.
**Warning signs:** Blendshape names from JSON don't match mesh names (e.g., "JawDrop" vs "Body_geo.JAW_DROP").

## Code Examples

### Example 1: Cached Blendshape Index Map (FaceAnimationPlayer Init)

```csharp
// Source: Unity API - Mesh.GetBlendShapeIndex + SkinnedMeshRenderer.SetBlendShapeWeight
// Build once at Start(), reuse every frame

private SkinnedMeshRenderer _renderer;
private Dictionary<string, int> _blendshapeIndexMap;

private void BuildBlendshapeMap()
{
    _blendshapeIndexMap = new Dictionary<string, int>();
    Mesh mesh = _renderer.sharedMesh;

    for (int i = 0; i < mesh.blendShapeCount; i++)
    {
        string name = mesh.GetBlendShapeName(i);
        // Store both original and normalized name for fuzzy lookup
        _blendshapeIndexMap[name] = i;

        string normalized = NormalizeBlendshapeName(name);
        if (!_blendshapeIndexMap.ContainsKey(normalized))
            _blendshapeIndexMap[normalized] = i;
    }
}

// Reuse NormalizeBlendshapeName logic from BlendshapeAnimationConverter
private static string NormalizeBlendshapeName(string name)
{
    int lastDot = name.LastIndexOf('.');
    if (lastDot >= 0 && lastDot < name.Length - 1)
        name = name.Substring(lastDot + 1);
    return name.ToLowerInvariant().Replace("_", "").Replace("-", "").Replace(" ", "");
}
```

### Example 2: Applying a BlendshapeFrame in Update()

```csharp
// Source: Unity API - SkinnedMeshRenderer.SetBlendShapeWeight

private void ApplyFrame(BlendshapeFrame frame)
{
    foreach (var kvp in frame.blendshapes)
    {
        string normalized = NormalizeBlendshapeName(kvp.Key);
        if (_blendshapeIndexMap.TryGetValue(normalized, out int index))
        {
            _renderer.SetBlendShapeWeight(index, kvp.Value * 100f); // 0-1 -> 0-100
        }
    }
}
```

### Example 3: Time-Accumulator Frame Streaming (FakeAnimationModel)

```csharp
// Pattern: Budget-based frame pacing synchronized to audio duration

private BlendshapeAnimationData _data;
private int _currentFrameIndex;
private float _animationBudgetSeconds; // Time budget from received audio
private float _timeAccumulator;
private float _frameDuration; // 1/fps from JSON data
private Action<BlendshapeFrame> _onFrameReady;

public void ProcessAudio(float[] audioSamples, int sampleRate)
{
    float audioDuration = (float)audioSamples.Length / sampleRate;
    _animationBudgetSeconds += audioDuration;
}

/// <summary>
/// Called by the owning MonoBehaviour every frame with Time.deltaTime.
/// Drains animation budget by emitting frames at the JSON's native FPS.
/// </summary>
public void Tick(float deltaTime)
{
    if (_animationBudgetSeconds <= 0f) return;

    _timeAccumulator += deltaTime;

    while (_timeAccumulator >= _frameDuration && _animationBudgetSeconds > 0f)
    {
        // Emit next frame
        var frame = _data.frames[_currentFrameIndex % _data.frames.Count];
        _onFrameReady?.Invoke(frame);

        _currentFrameIndex++;
        _timeAccumulator -= _frameDuration;
        _animationBudgetSeconds -= _frameDuration;
    }
}

public void Cancel()
{
    _animationBudgetSeconds = 0f;
    _timeAccumulator = 0f;
}
```

### Example 4: Sample App Wiring (AyaSampleController Integration)

```csharp
// In AyaSampleController or a new controller class:

private Audio2Animation _audio2Animation;
private FaceAnimationPlayer _facePlayer; // MonoBehaviour on face rig

private void Start()
{
    // Create fake model with pre-recorded data
    var jsonText = Resources.Load<TextAsset>("animDemo1"); // or load from SampleData
    var animData = BlendshapeAnimationConverter.ParseJson(jsonText.text);
    var fakeModel = new FakeAnimationModel(animData);

    _audio2Animation = new Audio2Animation(fakeModel);
    _audio2Animation.OnFrameReady += _facePlayer.EnqueueFrame;

    _session.OnSyncPacket += HandleSyncPacketForAnimation;
    _session.OnInterrupted += HandleInterruptedForAnimation;
}

private void HandleSyncPacketForAnimation(SyncPacket packet)
{
    _audio2Animation.ProcessPacket(packet);
}

private void HandleInterruptedForAnimation()
{
    _audio2Animation.Cancel();
    _facePlayer.ClearQueue();
}
```

## State of the Art

| Old Approach | Current Approach | Impact |
|--------------|------------------|--------|
| AnimationClip batch creation (BlendshapeAnimationConverter) | Direct SetBlendShapeWeight per frame | Required for real-time streaming; clips are for offline/preview |
| Single ISyncDriver assumed | Downstream consumer pattern | Audio2Animation doesn't interfere with existing sync pipeline |
| Whole-animation playback | Frame-by-frame streaming with audio budget | Enables future real ML model integration |

**Important context about existing data files:**
- animDemo1.json: 30fps, 269 frames, 8.97s, 63 blendshapes, 4 bones -- shortest, good for testing
- animDemo2.json: 25fps, 1786 frames, 71.44s, 63 blendshapes -- long
- animDemo3.json: 25fps, 3106 frames, 124.24s, 63 blendshapes -- longest
- animDemo4.json: 25fps, 3106 frames, 124.24s, 34 blendshapes -- fewer blendshapes

**Note:** animDemo1 uses 30fps while animDemo2-4 use 25fps. The FakeAnimationModel must read `timeSeconds` from the first frame to determine native FPS rather than hardcoding.

## Open Questions

1. **How to load SampleData JSON from the package at runtime?**
   - The SampleData files are in `Packages/com.google.ai-embodiment/Runtime/SampleData/`. TextAsset loading via `Resources.Load` only works for files in `Resources/` folders.
   - Options: (a) Use `UnityEngine.AddressableAssets` (overkill), (b) mark JSON files as TextAsset and reference via SerializedField, (c) use `System.IO.File.ReadAllText` with package path resolution, (d) embed as TextAsset resources in the package.
   - Recommendation: Use `[SerializeField] private TextAsset[] _animationDataFiles;` on the sample MonoBehaviour and assign in Inspector. Simplest, no path resolution needed. The package can also provide a static helper.

2. **Multi-mesh support in FaceAnimationPlayer?**
   - BlendshapeAnimationConverter.CreateAnimationClipForHierarchy handles multiple SkinnedMeshRenderers. Should FaceAnimationPlayer also scan children for multiple renderers?
   - Recommendation: YES -- build the same multi-mesh pattern. Cache index maps for ALL SkinnedMeshRenderers under the face rig root. Apply same blendshape values to all matching meshes.

3. **Bone animation (neck, head, eyes) in real-time?**
   - The pre-recorded data includes bone rotation frames (4 bones: neck, head, left eye, right eye).
   - For Phase 15.1, recommend SKIPPING bone animation and focusing only on blendshapes. Bone animation via `Transform.localRotation` requires finding the actual bone Transforms in the hierarchy, which adds complexity. Can be added in a follow-up.
   - Alternatively, if trivial, include it (same Tick pattern, `Transform.localRotation = frame.ToQuaternion()`).

4. **Does Audio2Animation need to be a MonoBehaviour or plain C#?**
   - Following PacketAssembler/ChirpTTSClient pattern: plain C# class.
   - But FakeAnimationModel needs a Tick() call driven by Update(). Who calls it?
   - Recommendation: FaceAnimationPlayer (MonoBehaviour in sample app) calls `_fakeModel.Tick(Time.deltaTime)` in its Update(). OR Audio2Animation exposes a Tick method that the sample controller calls. The package runtime class stays pure C#.

## Sources

### Primary (HIGH confidence)
- `Packages/com.google.ai-embodiment/Runtime/ISyncDriver.cs` -- interface design, single-driver architecture
- `Packages/com.google.ai-embodiment/Runtime/PacketAssembler.cs` -- `_syncDriver` single field (line 24), RegisterSyncDriver replaces (line 59-67)
- `Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs` -- OnSyncPacket event, RegisterSyncDriver passthrough, audio flow
- `Packages/com.google.ai-embodiment/Runtime/BlendshapeAnimationData.cs` -- BlendshapeFrame data model
- `Packages/com.google.ai-embodiment/Runtime/BlendshapeAnimationConverter.cs` -- ParseJson, fuzzy matching, weight * 100 conversion
- `Packages/com.google.ai-embodiment/Runtime/SyncPacket.cs` -- Audio field (float[] at 24kHz mono)
- `Packages/com.google.ai-embodiment/Runtime/SampleData/animDemo*.json` -- pre-recorded data analysis (frame counts, FPS, durations)
- [Unity Mesh.GetBlendShapeIndex](https://docs.unity3d.com/ScriptReference/Mesh.GetBlendShapeIndex.html) -- returns int index, -1 if not found
- [Unity SkinnedMeshRenderer.SetBlendShapeWeight](https://docs.unity3d.com/ScriptReference/SkinnedMeshRenderer.SetBlendShapeWeight.html) -- SetBlendShapeWeight(int index, float value), range 0-100

### Secondary (MEDIUM confidence)
- [uLipSync pattern](https://github.com/hecomi/uLipSync) -- real-time blendshape application via SetBlendShapeWeight in Update, similar architecture
- Unity coroutine/WaitForSeconds documentation -- frame timing patterns

### Tertiary (LOW confidence)
- None -- all findings verified against codebase and official Unity docs

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- all components exist in codebase, verified by reading source
- Architecture: HIGH -- ISyncDriver single-driver limitation verified in PacketAssembler.cs source code
- Pitfalls: HIGH -- all derived from verified codebase analysis and Unity API docs
- Frame timing/sync: MEDIUM -- the budget-based time accumulator is a well-known game dev pattern but specific tuning may need iteration
- SampleData loading: MEDIUM -- best approach depends on Unity editor workflow preferences

**Research date:** 2026-02-17
**Valid until:** 2026-03-17 (stable -- Unity APIs and package runtime architecture unlikely to change)
