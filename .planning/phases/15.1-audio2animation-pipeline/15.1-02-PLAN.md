---
phase: 15.1-audio2animation-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["15.1-01"]
files_modified:
  - Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/FaceAnimationPlayer.cs
autonomous: true

must_haves:
  truths:
    - "FaceAnimationPlayer receives BlendshapeFrame events and applies them to SkinnedMeshRenderer in real-time"
    - "Blendshape weights are converted from 0-1 (JSON) to 0-100 (Unity) scale"
    - "Blendshape name-to-index mapping is cached at initialization, not looked up per frame"
    - "FaceAnimationPlayer drives FakeAnimationModel.Tick() with Time.deltaTime every frame"
    - "FaceAnimationPlayer supports multiple SkinnedMeshRenderers under the face rig root"
    - "Frame queue is cleared on barge-in cancellation"
    - "Fuzzy blendshape name matching handles naming variations between JSON data and mesh (e.g., JawDrop vs Body_geo.JAW_DROP)"
  artifacts:
    - path: "Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/FaceAnimationPlayer.cs"
      provides: "MonoBehaviour applying streaming blendshape frames to face rig in real-time"
      exports: ["FaceAnimationPlayer"]
  key_links:
    - from: "FaceAnimationPlayer.cs"
      to: "Audio2Animation.OnFrameReady"
      via: "event subscription"
      pattern: "OnFrameReady.*EnqueueFrame"
    - from: "FaceAnimationPlayer.cs"
      to: "SkinnedMeshRenderer.SetBlendShapeWeight"
      via: "direct API call in Update"
      pattern: "SetBlendShapeWeight"
    - from: "FaceAnimationPlayer.cs"
      to: "BlendshapeAnimationConverter.NormalizeBlendshapeName"
      via: "static method call for fuzzy matching"
      pattern: "NormalizeBlendshapeName"
    - from: "FaceAnimationPlayer.cs"
      to: "FakeAnimationModel.Tick"
      via: "Update() drives tick with Time.deltaTime"
      pattern: "Tick.*Time\\.deltaTime"
---

<objective>
Create FaceAnimationPlayer MonoBehaviour that consumes BlendshapeFrame events from Audio2Animation and applies them in real-time to SkinnedMeshRenderer(s) on the Android XR face rig. This component drives FakeAnimationModel.Tick(), manages a frame queue, caches blendshape name-to-index mappings, and handles the 0-1 to 0-100 weight conversion. It also wires Audio2Animation to PersonaSession.OnSyncPacket for the complete pipeline.

Purpose: Completes the Audio2Animation pipeline by providing the sample-layer consumer that turns streaming frame data into visible face animation on the Android XR OpenXR face rig.
Output: 1 new MonoBehaviour in the sample app that connects the full pipeline.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15.1-audio2animation-pipeline/15.1-RESEARCH.md
@.planning/phases/15.1-audio2animation-pipeline/15.1-01-SUMMARY.md

Key source files:
@Packages/com.google.ai-embodiment/Runtime/Audio2Animation.cs
@Packages/com.google.ai-embodiment/Runtime/IAnimationModel.cs
@Packages/com.google.ai-embodiment/Runtime/FakeAnimationModel.cs
@Packages/com.google.ai-embodiment/Runtime/BlendshapeAnimationConverter.cs
@Packages/com.google.ai-embodiment/Runtime/BlendshapeAnimationData.cs
@Packages/com.google.ai-embodiment/Runtime/PersonaSession.cs
@Packages/com.google.ai-embodiment/Runtime/SyncPacket.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: FaceAnimationPlayer MonoBehaviour</name>
  <files>
    Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/FaceAnimationPlayer.cs
  </files>
  <action>
    Create a MonoBehaviour in the AIEmbodiment namespace (or AIEmbodiment.Samples if the sample assembly uses a different namespace -- check existing sample files first).

    **Inspector fields:**
    - `[SerializeField] private PersonaSession _personaSession;` -- to subscribe to OnSyncPacket and OnInterrupted
    - `[SerializeField] private Transform _faceRigRoot;` -- root transform to scan for SkinnedMeshRenderers (the Android XR face rig root)
    - `[SerializeField] private TextAsset[] _animationDataFiles;` -- pre-recorded JSON animation data files (assign animDemo1-4.json in Inspector). Use the longest available by default (pick first non-null).
    - `[Tooltip("Which animation data file index to use (0-based). Defaults to 0.")]`
      `[SerializeField] private int _activeDataIndex = 0;`

    **Private fields:**
    - `private Audio2Animation _audio2Animation;`
    - `private FakeAnimationModel _fakeModel;`
    - `private readonly Queue<BlendshapeFrame> _frameQueue = new Queue<BlendshapeFrame>();`
    - `private List<RendererMapping> _rendererMappings;` -- cached blendshape index maps for all SkinnedMeshRenderers

    **Helper struct (private inner):**
    ```csharp
    private struct RendererMapping
    {
        public SkinnedMeshRenderer Renderer;
        public Dictionary<string, int> BlendshapeIndexMap; // normalized name -> index
    }
    ```

    **Awake():**
    1. Validate _personaSession, _faceRigRoot, _animationDataFiles are assigned. Log errors and return if not.
    2. Clamp _activeDataIndex to valid range.
    3. Parse animation data: `var animData = BlendshapeAnimationConverter.ParseJson(_animationDataFiles[_activeDataIndex].text);`
    4. Create FakeAnimationModel: `_fakeModel = new FakeAnimationModel(animData);`
    5. Create Audio2Animation: `_audio2Animation = new Audio2Animation(_fakeModel);`
    6. Subscribe: `_audio2Animation.OnFrameReady += EnqueueFrame;`
    7. Build renderer mappings by calling `BuildRendererMappings()`.

    **OnEnable():**
    1. Subscribe to PersonaSession events:
       - `_personaSession.OnSyncPacket += HandleSyncPacket;`
       - `_personaSession.OnInterrupted += HandleInterrupted;`

    **OnDisable():**
    1. Unsubscribe from PersonaSession events:
       - `_personaSession.OnSyncPacket -= HandleSyncPacket;`
       - `_personaSession.OnInterrupted -= HandleInterrupted;`

    **BuildRendererMappings():**
    1. Find all SkinnedMeshRenderers under _faceRigRoot: `var renderers = _faceRigRoot.GetComponentsInChildren<SkinnedMeshRenderer>();`
    2. For each renderer with a non-null sharedMesh:
       - Create a `Dictionary<string, int>` mapping
       - For each blendshape on the mesh (0 to mesh.blendShapeCount):
         - Get name: `mesh.GetBlendShapeName(i)`
         - Store original: `map[name] = i`
         - Store normalized: `string normalized = BlendshapeAnimationConverter.NormalizeBlendshapeName(name); if (!map.ContainsKey(normalized)) map[normalized] = i;`
       - Add RendererMapping to _rendererMappings
    3. Log summary: number of renderers found, total blendshapes cached.

    **HandleSyncPacket(SyncPacket packet):**
    - Forward to Audio2Animation: `_audio2Animation.ProcessPacket(packet);`

    **HandleInterrupted():**
    - Cancel animation: `_audio2Animation.Cancel();`
    - Clear frame queue: `ClearQueue();`

    **EnqueueFrame(BlendshapeFrame frame):**
    - `_frameQueue.Enqueue(frame);`

    **ClearQueue():**
    - `_frameQueue.Clear();`
    - Also reset all blendshape weights to 0 on all renderers (return face to neutral):
      ```csharp
      foreach (var mapping in _rendererMappings)
      {
          for (int i = 0; i < mapping.Renderer.sharedMesh.blendShapeCount; i++)
              mapping.Renderer.SetBlendShapeWeight(i, 0f);
      }
      ```

    **Update():**
    1. Drive FakeAnimationModel tick: `_fakeModel?.Tick(Time.deltaTime);`
    2. Drain frame queue (1 frame per Update to match display rate, or drain all if queue is building up):
       ```csharp
       // Drain up to a reasonable number of frames per Update to prevent queue buildup
       // but typically only 1 frame is ready per Update at 30fps animation / 60+ fps display
       int maxFramesPerUpdate = 3;
       int framesApplied = 0;
       while (_frameQueue.Count > 0 && framesApplied < maxFramesPerUpdate)
       {
           var frame = _frameQueue.Dequeue();
           ApplyFrame(frame);
           framesApplied++;
       }
       ```

    **ApplyFrame(BlendshapeFrame frame):**
    - For each renderer mapping in _rendererMappings:
      - For each blendshape in frame.blendshapes:
        - Normalize the key: `string normalized = BlendshapeAnimationConverter.NormalizeBlendshapeName(kvp.Key);`
        - Look up index: `if (mapping.BlendshapeIndexMap.TryGetValue(normalized, out int index))`
        - Apply weight: `mapping.Renderer.SetBlendShapeWeight(index, kvp.Value * 100f);` (0-1 to 0-100 conversion -- Pitfall 3)

    **IMPORTANT NOTES:**
    - Do NOT create AnimationClips. Use direct SetBlendShapeWeight calls (real-time streaming pattern, not batch).
    - The Android XR face rig uses ARKit-compatible blendshape naming, which matches the animDemo*.json data. Fuzzy matching handles minor variations.
    - `_fakeModel.Tick()` is called BEFORE draining the queue in Update() so that newly produced frames are immediately available.
    - The 0-1 to 0-100 weight conversion is critical. JSON values are 0-1, Unity SetBlendShapeWeight expects 0-100.
  </action>
  <verify>
    File exists at `Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/FaceAnimationPlayer.cs`. Verify it:
    1. Has [SerializeField] for PersonaSession, Transform faceRigRoot, TextAsset[] animationDataFiles
    2. Creates Audio2Animation and FakeAnimationModel in Awake
    3. Subscribes to OnSyncPacket and OnInterrupted in OnEnable
    4. Calls _fakeModel.Tick(Time.deltaTime) in Update
    5. Applies frames via SetBlendShapeWeight with * 100f conversion
    6. Uses BlendshapeAnimationConverter.NormalizeBlendshapeName for fuzzy matching
    7. ClearQueue resets weights to 0 (neutral face)
  </verify>
  <done>
    FaceAnimationPlayer receives streaming BlendshapeFrame events, queues them, and applies to all SkinnedMeshRenderers under the face rig root in Update(). Blendshape index maps are cached at init. Weights convert 0-1 to 0-100. Barge-in clears queue and resets face to neutral. FakeAnimationModel.Tick() is driven each frame. Pipeline is fully wired: PersonaSession.OnSyncPacket -> Audio2Animation -> FakeAnimationModel -> OnFrameReady -> FaceAnimationPlayer -> SetBlendShapeWeight.
  </done>
</task>

</tasks>

<verification>
1. FaceAnimationPlayer.cs exists in `Packages/com.google.ai-embodiment/Samples~/AyaLiveStream/`
2. Full pipeline chain is wired: OnSyncPacket -> Audio2Animation.ProcessPacket -> FakeAnimationModel.ProcessAudio -> Tick -> OnFrameReady -> EnqueueFrame -> Update -> ApplyFrame -> SetBlendShapeWeight
3. Weight conversion: frame values * 100f in ApplyFrame
4. Fuzzy matching: BlendshapeAnimationConverter.NormalizeBlendshapeName used for index lookup
5. Multi-mesh support: scans all SkinnedMeshRenderers under _faceRigRoot
6. Cancel path: OnInterrupted -> Audio2Animation.Cancel + ClearQueue (neutral face reset)
7. No AnimationClip creation (direct SetBlendShapeWeight only)
</verification>

<success_criteria>
- FaceAnimationPlayer is a complete, self-contained MonoBehaviour ready to attach to a GameObject
- Inspector configuration: assign PersonaSession, face rig root Transform, and TextAsset animation data files
- Full pipeline from audio to face animation works end-to-end when PersonaSession receives SyncPackets
- Barge-in cancellation clears animation and resets face to neutral
- No per-frame string allocations for blendshape name lookup (cached at init)
</success_criteria>

<output>
After completion, create `.planning/phases/15.1-audio2animation-pipeline/15.1-02-SUMMARY.md`
</output>
